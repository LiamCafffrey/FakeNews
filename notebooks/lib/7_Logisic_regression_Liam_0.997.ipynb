{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re,string\n",
    "import tensorflow as tf\n",
    "import enchant\n",
    "import trafilatura\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = pd.read_csv('../raw_data/True.csv')\n",
    "fake = pd.read_csv('../raw_data/Fake.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting non necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "true.drop(columns = ['subject','date'], inplace = True)\n",
    "fake.drop(columns = ['subject','date'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief data cleaning to fake dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['/Getty Images']\n",
    "pat = '|'.join(r\"\\b{}\\b\".format(x) for x in stop_words)\n",
    "fake['text'] = fake['text'].str.replace(pat, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "true['score'] = 1\n",
    "fake['score'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate everything to one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([true,fake],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing links from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def rem_urls(text):\n",
    "    return re.sub('https?:\\S+','',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['title_clean']=data['title'].apply(rem_urls)\n",
    "data['text_clean']=data['text'].apply(rem_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "punc_no_sq = '!“#$%&\\()*+,./:;<=>?@[\\\\]^_`{|}~“”—’-'\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    for punctuation in punc_no_sq:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['title_clean']=data['title_clean'].apply(remove_punctuation)\n",
    "data['text_clean']=data['text_clean'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    text = ''.join(word for word in text if not word.isdigit())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['title_clean']=data['title_clean'].apply(remove_numbers)\n",
    "\n",
    "data['text_clean']=data['text_clean'].apply(remove_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making everything lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(text):\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['title_clean']=data['title_clean'].apply(lower_case)\n",
    "data['text_clean']=data['text_clean'].apply(lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "data['title_clean'] = data['title_clean'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop_words)]))\n",
    "data['text_clean'] = data['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizing text in order to count how many words we have to calculate ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    return text.split()\n",
    "\n",
    "data['title_tokens']=data['title_clean'].apply(tokenize_text)\n",
    "data['text_tokens']=data['text_clean'].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Function to count typos in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "english = enchant.DictWithPWL(\"en_US\", \"vocab.txt\")\n",
    "wrong_words={}\n",
    "correct_words=set()\n",
    "def get_typos_t(tokens):\n",
    "     wrong_count=0\n",
    "     for token in tokens:\n",
    "            if token in wrong_words:\n",
    "                wrong_words[token]+=1\n",
    "                wrong_count+=1\n",
    "            else:\n",
    "                if not token in correct_words:\n",
    "                    if token[0].islower() and not '-' in token and not english.check(token) and not english.check(token.capitalize()):\n",
    "                        wrong_words[token]=1\n",
    "                        wrong_count+=1\n",
    "                    else:\n",
    "                        correct_words.add(token)\n",
    "     return wrong_count    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create typo ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['typos_title_count']=data['title_tokens'].apply(get_typos_t)\n",
    "data['typos_text_count']=data['text_tokens'].apply(get_typos_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['title_typo_ratio']= data['typos_title_count']/len(data['title_tokens'])\n",
    "data['text_typo_ratio']= data['typos_text_count']/len(data['text_tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordering the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['title', 'text', 'title_clean', 'text_clean', 'title_tokens',\n",
    "       'text_tokens', 'typos_title_count', 'typos_text_count',\n",
    "       'title_typo_ratio', 'text_typo_ratio','score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Generic Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def preparation(data):\n",
    "    data['title_clean']=data['title'].apply(remove_numbers)\n",
    "    data['text_clean']=data['text'].apply(remove_numbers)\n",
    "    \n",
    "    data['title_clean']=data['title_clean'].apply(remove_punctuation)\n",
    "    data['text_clean']=data['text_clean'].apply(remove_punctuation)\n",
    "    \n",
    "    data['title_clean'] = data['title_clean'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop_words)]))\n",
    "    data['text_clean'] = data['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop_words)]))\n",
    "    \n",
    "    data['title_tokens']=data['title_clean'].apply(tokenize_text)\n",
    "    data['text_tokens']=data['text_clean'].apply(tokenize_text)\n",
    "    \n",
    "    data['title_token_count']=data['title_tokens'].apply(lambda tokens:len(tokens))\n",
    "    data['text_token_count']=data['text_tokens'].apply(lambda tokens:len(tokens))\n",
    "    \n",
    "    data['wrong_title_token_count']=data['title_tokens'].apply(get_typos_t)\n",
    "    data['wrong_text_token_count']=data['text_tokens'].apply(get_typos_t)\n",
    "    \n",
    "    data['title_typo_ratio']= data['wrong_title_token_count']/data['title_token_count']\n",
    "    data['text_typo_ratio']= data['wrong_text_token_count']/data['text_token_count']\n",
    "    \n",
    "    return data[['title_clean', 'text_clean','text_typo_ratio']]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to run a cross_val and find the best params for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## separating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[['title_clean', 'text_clean','text_typo_ratio']]\n",
    "y=data['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data in two : Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=0,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a pipeline to vectorize the text/title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "preprocessor = ColumnTransformer([\n",
    "    \n",
    "    ('vectorizer_title', CountVectorizer(), 'title_clean'),\n",
    "    ('vectorizer_text', CountVectorizer(), 'text_clean'),\n",
    "\n",
    "    #insert function here\n",
    "    \n",
    "])\n",
    "\n",
    "final_pipe = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('Logistic', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid searching for best params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    \n",
    "    'Logistic__solver': ('newton-cg', 'lbfgs', 'sag'),\n",
    "    'Logistic__C': ([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(final_pipe,\n",
    "                           parameters,\n",
    "                           scoring = [\"f1\", \"accuracy\", \"recall\"], \n",
    "                           refit= \"accuracy\",\n",
    "                           cv=3,\n",
    "                           verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\liamc\\.venvs\\lewagon\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\liamc\\.venvs\\lewagon\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\liamc\\.venvs\\lewagon\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\liamc\\.venvs\\lewagon\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\liamc\\.venvs\\lewagon\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "grid_search.fit(x_train,y_train)\n",
    "stop = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic__C': 0.4, 'Logistic__solver': 'newton-cg'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ok so now that we have best params lets train a model on them**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model on best params found in grid_search with reuters in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    \n",
    "    ('vectorizer_title', CountVectorizer(), 'title_clean'),\n",
    "    ('vectorizer_text', CountVectorizer(), 'text_clean'),\n",
    "\n",
    "    #insert function here\n",
    "])\n",
    "\n",
    "final_pipe = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('Logistic', LogisticRegression(solver = 'newton-cg', C =0.4 ))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('preprocessing', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('vectorizer_title', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', ...ty='l2', random_state=None, solver='newton-cg',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pipe.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9973273942093541"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pipe.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this final pipe is fitted with training data cointaing reuters in the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing our model on some new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a function that cleans the test text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import trafilatura\n",
    "downloaded = trafilatura.fetch_url('https://www.rte.ie/news/ireland/2020/1121/1179617-zoo-funding/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test = trafilatura.extract(downloaded)\n",
    "title_test='€1.6m in funding secured to support zoo sector'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "punc = string.punctuation + '“' + '”' + '’' + '‘' +'—' +'€' + '❤'\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    for punctuation in punc:\n",
    "        text = text.replace(punctuation, '')\n",
    "    text = ''.join(word for word in text if not word.isdigit())\n",
    "    text = text.lower()\n",
    "    text_split = text.split()\n",
    "    for word in text_split:\n",
    "        if word in stop_words:\n",
    "            text_split.remove(word)\n",
    "    text_string = ''\n",
    "    for word in text_split:\n",
    "        text_string = text_string + ' ' + word\n",
    "    return text_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clean = clean_text(text_test)\n",
    "title_clean = clean_text(title_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a general function to calculate typo ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def typo_ratio(text):\n",
    "    text_split = text.split()\n",
    "    lenght = len(text_split)\n",
    "    typos = get_typos_t(text_split)\n",
    "    return typos/lenght"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the data as an input that the model accepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_series=pd.Series(text_clean)\n",
    "title_series=pd.Series(title_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df=pd.DataFrame({'title_clean':title_series,'text_clean':test_series, 'text_typo_ratio': typo_ratio(text_clean)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pipe.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how we use the model to predict on one specific example(this pipe is fitted with reuters text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying a logistic regression with real news not having reuters in its text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets create a new column in data that will have the same text/title without reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['reuters']\n",
    "pat = '|'.join(r\"\\b{}\\b\".format(x) for x in stop_words)\n",
    "data['text_clean_without_reuters'] = data['text_clean'].str.replace(pat, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = data[['title_clean', 'text_clean_without_reuters','text_typo_ratio']]\n",
    "y_2 = data['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train_2,x_test_2,y_train_2,y_test_2=train_test_split(x_2,y_2,random_state=0,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "preprocessor_2 = ColumnTransformer([\n",
    "    \n",
    "    ('vectorizer_title', CountVectorizer(), 'title_clean'),\n",
    "    ('vectorizer_text', CountVectorizer(), 'text_clean_without_reuters'),\n",
    "\n",
    "    #insert function here\n",
    "    \n",
    "])\n",
    "\n",
    "final_pipe_2 = Pipeline([\n",
    "    ('preprocessing', preprocessor_2),\n",
    "    ('Logistic', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    \n",
    "    'Logistic__solver': ('newton-cg', 'lbfgs'), #didnt try sag because it wasnt converging\n",
    "    'Logistic__C': ([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "\n",
    "}\n",
    "\n",
    "grid_search_2 = GridSearchCV(final_pipe_2,\n",
    "                           parameters,\n",
    "                           scoring = [\"f1\", \"accuracy\", \"recall\"], \n",
    "                           refit= \"accuracy\",\n",
    "                           cv=3,\n",
    "                           verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\liamc\\.venvs\\lewagon\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\liamc\\.venvs\\lewagon\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\liamc\\.venvs\\lewagon\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\liamc\\.venvs\\lewagon\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\liamc\\.venvs\\lewagon\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\liamc\\.venvs\\lewagon\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\liamc\\.venvs\\lewagon\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\liamc\\.venvs\\lewagon\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('preprocessing', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('vectorizer_title', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', ...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'Logistic__solver': ('newton-cg', 'lbfgs'), 'Logistic__C': [0.2, 0.4, 0.6, 0.8, 1.0]},\n",
       "       pre_dispatch='2*n_jobs', refit='accuracy',\n",
       "       return_train_score='warn', scoring=['f1', 'accuracy', 'recall'],\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_2.fit(x_train_2,y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9887679775995927"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic__C': 0.4, 'Logistic__solver': 'newton-cg'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok so even without reuters in the text the best params remain the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training  a model on best params found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "preprocessor_2 = ColumnTransformer([\n",
    "    \n",
    "    ('vectorizer_title', CountVectorizer(), 'title_clean'),\n",
    "    ('vectorizer_text', CountVectorizer(), 'text_clean_without_reuters'),\n",
    "\n",
    "    #insert function here  \n",
    "    \n",
    "])\n",
    "\n",
    "final_pipe_2 = Pipeline([\n",
    "    ('preprocessing', preprocessor_2),\n",
    "    ('Logistic', LogisticRegression(solver = 'newton-cg', C =0.4 ))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('preprocessing', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('vectorizer_title', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', ...ty='l2', random_state=None, solver='newton-cg',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pipe_2.fit(x_train_2,y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9925760950259837"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pipe_2.score(x_test_2,y_test_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the same example as before where we got fake when the news was real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded = trafilatura.fetch_url('https://www.bbc.com/news/av/uk-england-leicestershire-55007262')\n",
    "text_test_2 = trafilatura.extract(downloaded)\n",
    "title_test_2 ='''Horse racing: 'It doesn't matter what colour you are'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clean_2 = clean_text(text_test_2)\n",
    "title_clean_2 = clean_text(title_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_series_2=pd.Series(text_clean_2)\n",
    "title_series_2=pd.Series(title_clean_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_2 = pd.DataFrame({'title_clean':title_series_2,'text_clean_without_reuters':test_series_2, 'text_typo_ratio': typo_ratio(text_clean_2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import classification_report\n",
    "#print(classification_report(y_true, y_pred, target_names=target_names)) work on this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pipe_2.predict(test_df_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**okay so this model is generalising alot better**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking how well the model generalise for fake news using the fake news only dataset (testing in pipeline fitted with reuters(pipeline) and fitted witout it (pipeline_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_test_pipeline_2 = pd.read_csv('../raw_data/fake_extra.csv')\n",
    "fake_test_pipeline_2.dropna(axis = 0, inplace = True)\n",
    "fake_test_pipeline_2 = fake_test_pipeline_2[['title','text']]\n",
    "fake_test_pipeline_2['text_clean_without_reuters'] = fake_test_pipeline_2['text'].apply(remove_punctuation)\n",
    "fake_test_pipeline_2['title_clean'] = fake_test_pipeline_2['title'].apply(remove_punctuation)\n",
    "fake_test_pipeline_2['text_clean_without_reuters'] = fake_test_pipeline_2['text_clean_without_reuters'].apply(remove_numbers)\n",
    "fake_test_pipeline_2['title_clean'] = fake_test_pipeline_2['title_clean'].apply(remove_numbers)\n",
    "fake_test_pipeline_2['text_clean_without_reuters'] = fake_test_pipeline_2['text_clean_without_reuters'].apply(lower_case)\n",
    "fake_test_pipeline_2['title_clean'] = fake_test_pipeline_2['title_clean'].apply(lower_case)\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "fake_test_pipeline_2['text_clean_without_reuters'] = fake_test_pipeline_2['text_clean_without_reuters'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop_words)]))\n",
    "fake_test_pipeline_2['title_clean'] = fake_test_pipeline_2['title_clean'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop_words)]))\n",
    "fake_test_pipeline_2['title_tokens']=fake_test_pipeline_2['title_clean'].apply(tokenize_text)\n",
    "fake_test_pipeline_2['text_tokens']=fake_test_pipeline_2['text_clean_without_reuters'].apply(tokenize_text)\n",
    "fake_test_pipeline_2['typos_title_count'] = fake_test_pipeline_2['title_tokens'].apply(get_typos_t)\n",
    "fake_test_pipeline_2['typos_text_count'] = fake_test_pipeline_2['text_tokens'].apply(get_typos_t)\n",
    "fake_test_pipeline_2['title_typo_ratio']= fake_test_pipeline_2['typos_title_count']/len(fake_test_pipeline_2['title_tokens'])\n",
    "fake_test_pipeline_2['text_typo_ratio']= fake_test_pipeline_2['typos_text_count']/len(fake_test_pipeline_2['text_tokens'])\n",
    "fake_test_pipeline_2 = fake_test_pipeline_2[['title_clean','text_clean_without_reuters', 'text_typo_ratio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 4155, 1: 547}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_fake_without_reuters = final_pipe_2.predict(fake_test_pipeline_2)\n",
    "unique_fake_without_reuters, counts_fake_without_reuters = np.unique(result_fake_without_reuters, return_counts=True)\n",
    "dict(zip(unique_fake_without_reuters, counts_fake_without_reuters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8836665248830285"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4155/(4155+547)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the accuracy on fake dataset by the pipeline fitted without reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_test_pipeline = pd.read_csv('../raw_data/fake_extra.csv')\n",
    "fake_test_pipeline.dropna(axis = 0, inplace = True)\n",
    "fake_test_pipeline = fake_test_pipeline[['title','text']]\n",
    "fake_test_pipeline['text_clean'] = fake_test_pipeline['text'].apply(remove_punctuation)\n",
    "fake_test_pipeline['title_clean'] = fake_test_pipeline['title'].apply(remove_punctuation)\n",
    "fake_test_pipeline['text_clean'] = fake_test_pipeline['text_clean'].apply(remove_numbers)\n",
    "fake_test_pipeline['title_clean'] = fake_test_pipeline['title_clean'].apply(remove_numbers)\n",
    "fake_test_pipeline['text_clean'] = fake_test_pipeline['text_clean'].apply(lower_case)\n",
    "fake_test_pipeline['title_clean'] = fake_test_pipeline['title_clean'].apply(lower_case)\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "fake_test_pipeline['text_clean'] = fake_test_pipeline['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop_words)]))\n",
    "fake_test_pipeline['title_clean'] = fake_test_pipeline['title_clean'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop_words)]))\n",
    "fake_test_pipeline['title_tokens']=fake_test_pipeline['title_clean'].apply(tokenize_text)\n",
    "fake_test_pipeline['text_tokens']=fake_test_pipeline['text_clean'].apply(tokenize_text)\n",
    "fake_test_pipeline['typos_title_count'] = fake_test_pipeline['title_tokens'].apply(get_typos_t)\n",
    "fake_test_pipeline['typos_text_count'] = fake_test_pipeline['text_tokens'].apply(get_typos_t)\n",
    "fake_test_pipeline['title_typo_ratio']= fake_test_pipeline['typos_title_count']/len(fake_test_pipeline['title_tokens'])\n",
    "fake_test_pipeline['text_typo_ratio']= fake_test_pipeline['typos_text_count']/len(fake_test_pipeline['text_tokens'])\n",
    "fake_test_pipeline = fake_test_pipeline[['title_clean','text_clean', 'text_typo_ratio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 4546, 1: 156}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_fake_with_reuters = final_pipe.predict(fake_test_pipeline)\n",
    "unique_fake_with_reuters, counts_fake_with_reuters = np.unique(result_fake_with_reuters, return_counts=True)\n",
    "dict(zip(unique_fake_with_reuters, counts_fake_with_reuters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9668226286686517"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4546/(4546+156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the accuracy on fake daset by the pipeline fitted with reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing both pipelines on true news and checking which one perfoms best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_test_pipeline_2 = pd.read_csv('../raw_data/new_york_real.csv')\n",
    "true_test_pipeline_2.dropna(axis = 0, inplace = True)\n",
    "true_test_pipeline_2 = true_test_pipeline_2[['title','text']]\n",
    "true_test_pipeline_2['text_clean_without_reuters'] = true_test_pipeline_2['text'].apply(remove_punctuation)\n",
    "true_test_pipeline_2['title_clean'] = true_test_pipeline_2['title'].apply(remove_punctuation)\n",
    "true_test_pipeline_2['text_clean_without_reuters'] = true_test_pipeline_2['text_clean_without_reuters'].apply(remove_numbers)\n",
    "true_test_pipeline_2['title_clean'] = true_test_pipeline_2['title_clean'].apply(remove_numbers)\n",
    "true_test_pipeline_2['text_clean_without_reuters'] = true_test_pipeline_2['text_clean_without_reuters'].apply(lower_case)\n",
    "true_test_pipeline_2['title_clean'] = true_test_pipeline_2['title_clean'].apply(lower_case)\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "true_test_pipeline_2['text_clean_without_reuters'] = true_test_pipeline_2['text_clean_without_reuters'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop_words)]))\n",
    "true_test_pipeline_2['title_clean'] = true_test_pipeline_2['title_clean'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop_words)]))\n",
    "true_test_pipeline_2['title_tokens']=true_test_pipeline_2['title_clean'].apply(tokenize_text)\n",
    "true_test_pipeline_2['text_tokens']=true_test_pipeline_2['text_clean_without_reuters'].apply(tokenize_text)\n",
    "true_test_pipeline_2['typos_title_count'] = true_test_pipeline_2['title_tokens'].apply(get_typos_t)\n",
    "true_test_pipeline_2['typos_text_count'] = true_test_pipeline_2['text_tokens'].apply(get_typos_t)\n",
    "true_test_pipeline_2['title_typo_ratio']= true_test_pipeline_2['typos_title_count']/len(true_test_pipeline_2['title_tokens'])\n",
    "true_test_pipeline_2['text_typo_ratio']= true_test_pipeline_2['typos_text_count']/len(true_test_pipeline_2['text_tokens'])\n",
    "true_test_pipeline_2 = true_test_pipeline_2[['title_clean','text_clean_without_reuters', 'text_typo_ratio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 4878, 1: 2925}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_without_reuters_trained = final_pipe_2.predict(true_test_pipeline_2)\n",
    "unique, counts = np.unique(result_without_reuters_trained, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3748558246828143"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2925/(4878+2925)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the accuracy on true daset by the pipeline fitted without reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_test_for_pipeline_1 = pd.read_csv('../raw_data/new_york_real.csv')\n",
    "true_test_for_pipeline_1.dropna(axis = 0, inplace = True)\n",
    "true_test_for_pipeline_1 = true_test_for_pipeline_1[['title','text']]\n",
    "true_test_for_pipeline_1['text_clean'] = true_test_for_pipeline_1['text'].apply(remove_punctuation)\n",
    "true_test_for_pipeline_1['title_clean'] = true_test_for_pipeline_1['title'].apply(remove_punctuation)\n",
    "true_test_for_pipeline_1['text_clean'] = true_test_for_pipeline_1['text_clean'].apply(remove_numbers)\n",
    "true_test_for_pipeline_1['title_clean'] = true_test_for_pipeline_1['title_clean'].apply(remove_numbers)\n",
    "true_test_for_pipeline_1['text_clean'] = true_test_for_pipeline_1['text_clean'].apply(lower_case)\n",
    "true_test_for_pipeline_1['title_clean'] = true_test_for_pipeline_1['title_clean'].apply(lower_case)\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "true_test_for_pipeline_1['text_clean'] = true_test_for_pipeline_1['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop_words)]))\n",
    "true_test_for_pipeline_1['title_clean'] = true_test_for_pipeline_1['title_clean'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop_words)]))\n",
    "true_test_for_pipeline_1['title_tokens']=true_test_for_pipeline_1['title_clean'].apply(tokenize_text)\n",
    "true_test_for_pipeline_1['text_tokens']=true_test_for_pipeline_1['text_clean'].apply(tokenize_text)\n",
    "true_test_for_pipeline_1['typos_title_count'] = true_test_for_pipeline_1['title_tokens'].apply(get_typos_t)\n",
    "true_test_for_pipeline_1['typos_text_count'] = true_test_for_pipeline_1['text_tokens'].apply(get_typos_t)\n",
    "true_test_for_pipeline_1['title_typo_ratio']= true_test_for_pipeline_1['typos_title_count']/len(true_test_for_pipeline_1['title_tokens'])\n",
    "true_test_for_pipeline_1['text_typo_ratio']= true_test_for_pipeline_1['typos_text_count']/len(true_test_for_pipeline_1['text_tokens'])\n",
    "true_test_for_pipeline_1 = true_test_for_pipeline_1[['title_clean','text_clean', 'text_typo_ratio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 5938, 1: 1865}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_with_reuters = final_pipe.predict(true_test_for_pipeline_1)\n",
    "unique_2, counts_2 = np.unique(result_with_reuters, return_counts=True)\n",
    "dict(zip(unique_2, counts_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23901063693451235"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1865/(5938+1865)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the accuracy on true daset by the pipeline fitted with reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**the model that was trained without reuters perfoms better than the one that trained with reuters for detecting real news**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying a logistic regression with more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>title_tokens</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>typos_title_count</th>\n",
       "      <th>typos_text_count</th>\n",
       "      <th>title_typo_ratio</th>\n",
       "      <th>text_typo_ratio</th>\n",
       "      <th>score</th>\n",
       "      <th>text_clean_without_reuters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>us budget fight looms republicans flip fiscal ...</td>\n",
       "      <td>washington reuters head conservative republica...</td>\n",
       "      <td>[us, budget, fight, looms, republicans, flip, ...</td>\n",
       "      <td>[washington, reuters, head, conservative, repu...</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>1</td>\n",
       "      <td>washington  head conservative republican facti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>us military accept transgender recruits monday...</td>\n",
       "      <td>washington reuters transgender people allowed ...</td>\n",
       "      <td>[us, military, accept, transgender, recruits, ...</td>\n",
       "      <td>[washington, reuters, transgender, people, all...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>1</td>\n",
       "      <td>washington  transgender people allowed first t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>senior us republican senator 'let mr mueller job'</td>\n",
       "      <td>washington reuters special counsel investigati...</td>\n",
       "      <td>[senior, us, republican, senator, 'let, mr, mu...</td>\n",
       "      <td>[washington, reuters, special, counsel, invest...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>1</td>\n",
       "      <td>washington  special counsel investigation link...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>fbi russia probe helped australian diplomat ti...</td>\n",
       "      <td>washington reuters trump campaign adviser geor...</td>\n",
       "      <td>[fbi, russia, probe, helped, australian, diplo...</td>\n",
       "      <td>[washington, reuters, trump, campaign, adviser...</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>1</td>\n",
       "      <td>washington  trump campaign adviser george papa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>trump wants postal service charge 'much more' ...</td>\n",
       "      <td>seattlewashington reuters president donald tru...</td>\n",
       "      <td>[trump, wants, postal, service, charge, 'much,...</td>\n",
       "      <td>[seattlewashington, reuters, president, donald...</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>1</td>\n",
       "      <td>seattlewashington  president donald trump call...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...   \n",
       "\n",
       "                                         title_clean  \\\n",
       "0  us budget fight looms republicans flip fiscal ...   \n",
       "1  us military accept transgender recruits monday...   \n",
       "2  senior us republican senator 'let mr mueller job'   \n",
       "3  fbi russia probe helped australian diplomat ti...   \n",
       "4  trump wants postal service charge 'much more' ...   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  washington reuters head conservative republica...   \n",
       "1  washington reuters transgender people allowed ...   \n",
       "2  washington reuters special counsel investigati...   \n",
       "3  washington reuters trump campaign adviser geor...   \n",
       "4  seattlewashington reuters president donald tru...   \n",
       "\n",
       "                                        title_tokens  \\\n",
       "0  [us, budget, fight, looms, republicans, flip, ...   \n",
       "1  [us, military, accept, transgender, recruits, ...   \n",
       "2  [senior, us, republican, senator, 'let, mr, mu...   \n",
       "3  [fbi, russia, probe, helped, australian, diplo...   \n",
       "4  [trump, wants, postal, service, charge, 'much,...   \n",
       "\n",
       "                                         text_tokens  typos_title_count  \\\n",
       "0  [washington, reuters, head, conservative, repu...                  0   \n",
       "1  [washington, reuters, transgender, people, all...                  0   \n",
       "2  [washington, reuters, special, counsel, invest...                  1   \n",
       "3  [washington, reuters, trump, campaign, adviser...                  3   \n",
       "4  [seattlewashington, reuters, president, donald...                  1   \n",
       "\n",
       "   typos_text_count  title_typo_ratio  text_typo_ratio  score  \\\n",
       "0                15          0.000000         0.000334      1   \n",
       "1                10          0.000000         0.000223      1   \n",
       "2                 8          0.000022         0.000178      1   \n",
       "3                17          0.000067         0.000379      1   \n",
       "4                23          0.000022         0.000512      1   \n",
       "\n",
       "                          text_clean_without_reuters  \n",
       "0  washington  head conservative republican facti...  \n",
       "1  washington  transgender people allowed first t...  \n",
       "2  washington  special counsel investigation link...  \n",
       "3  washington  trump campaign adviser george papa...  \n",
       "4  seattlewashington  president donald trump call...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding different features that we saw were relevant to distinguish fake from real news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def stopwords_ratio(tokens):\n",
    "    count_stop_words = 0\n",
    "    amount_tokens = 0   \n",
    "    for token in tokens:\n",
    "        amount_tokens += 1\n",
    "        if token in stop_words:\n",
    "            count_stop_words += 1\n",
    "    if amount_tokens == 0:\n",
    "        return 0\n",
    "    return count_stop_words / amount_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['title_length_char'] = data.title.str.len() #amount of char in title\n",
    "data['title_Upper'] = data['title'].str.count(r'[A-Z]')\n",
    "data['title_Upper_Ratio'] = data['title_Upper']/data['title_length_char'] #ratio of Uppercase Char in title\n",
    "data.drop(columns = ['title_Upper'])\n",
    "data['tex_tokens_with_stopwords']=data['text'].apply(tokenize_text)\n",
    "data['text_stop_words_ratio'] = data['tex_tokens_with_stopwords'].apply(stopwords_ratio) #stopwords ratio in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train a model on the different features we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_for_test = data[['title_clean', 'text_clean_without_reuters','text_typo_ratio','title_length_char','title_Upper_Ratio','text_stop_words_ratio']]\n",
    "y_for_test = data['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x_for_test,y_for_test,random_state=0,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_clean</th>\n",
       "      <th>text_clean_without_reuters</th>\n",
       "      <th>text_typo_ratio</th>\n",
       "      <th>title_length_char</th>\n",
       "      <th>title_Upper_Ratio</th>\n",
       "      <th>text_stop_words_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33958</th>\n",
       "      <td>breaking finally new wikileaks email…we going ...</td>\n",
       "      <td>latest wikileaks email evidence smoke hillary ...</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>86</td>\n",
       "      <td>0.220930</td>\n",
       "      <td>0.420118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19813</th>\n",
       "      <td>german liberals would expect finance ministry ...</td>\n",
       "      <td>berlin  germany free democrats fdp would want ...</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>65</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>0.386935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25814</th>\n",
       "      <td>trump loses complete nervous breakdown worst w...</td>\n",
       "      <td>trump bad week first humiliated front millions...</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>82</td>\n",
       "      <td>0.317073</td>\n",
       "      <td>0.409214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18689</th>\n",
       "      <td>merkel macron pledge lead eu forward postbrexit</td>\n",
       "      <td>tallinn  french president emmanuel macron back...</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>52</td>\n",
       "      <td>0.096154</td>\n",
       "      <td>0.382979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44673</th>\n",
       "      <td>american tragedy really killed jonbenét ramsey</td>\n",
       "      <td>roses know thorns hurt quote attributed jonben...</td>\n",
       "      <td>0.005435</td>\n",
       "      <td>55</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.384701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22749</th>\n",
       "      <td>‘portal hell internet loses mysterious red lig...</td>\n",
       "      <td>earlier month sinkhole opened outside donald t...</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>99</td>\n",
       "      <td>0.161616</td>\n",
       "      <td>0.291312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2628</th>\n",
       "      <td>man whose firm behind trump dossier testify se...</td>\n",
       "      <td>washington  cofounder firm commissioned dossie...</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>71</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>0.422360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10117</th>\n",
       "      <td>biden ukraine's poroshenko meet thursday white...</td>\n",
       "      <td>washington  us vice president joe biden ukrain...</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>57</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.347222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4388</th>\n",
       "      <td>us strikes syria show resolve chemical attacks...</td>\n",
       "      <td>brussels  european council president donald tu...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.254545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16056</th>\n",
       "      <td>tunnel collapse may killed north korea nuclear...</td>\n",
       "      <td>tokyo  tunnel north korea nuclear test site co...</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>88</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.330935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>manafort attorney evidence client colluded russia</td>\n",
       "      <td>washington  attorney former trump campaign man...</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>58</td>\n",
       "      <td>0.051724</td>\n",
       "      <td>0.367647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12370</th>\n",
       "      <td>irish parliament committee backs referendum ab...</td>\n",
       "      <td>dublin  irish parliamentary committee wednesda...</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>70</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.453165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41275</th>\n",
       "      <td>sean hannity takes gloves hillary supporter me...</td>\n",
       "      <td>sean hannity taking page trump playbook member...</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>108</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.413919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2270</th>\n",
       "      <td>trump administration's africa policy focus ago...</td>\n",
       "      <td>washington  trump administrations trade agenda...</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>65</td>\n",
       "      <td>0.092308</td>\n",
       "      <td>0.370066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32755</th>\n",
       "      <td>crooked hillarys campaign manager wont rule ru...</td>\n",
       "      <td>mere months receiving end one stunning losses ...</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>134</td>\n",
       "      <td>0.253731</td>\n",
       "      <td>0.396226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31518</th>\n",
       "      <td>leftist antifa attacks boston police…shows tru...</td>\n",
       "      <td>hate antifa well left really want associate vi...</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>107</td>\n",
       "      <td>0.242991</td>\n",
       "      <td>0.126984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6615</th>\n",
       "      <td>factbox trump fills top jobs administration</td>\n",
       "      <td>us presidentelect donald trump wednesday name...</td>\n",
       "      <td>0.001403</td>\n",
       "      <td>52</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.333754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18256</th>\n",
       "      <td>venezuela's unrest food scarcity take psycholo...</td>\n",
       "      <td>los teques venezuela  venezuelan siblings jere...</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>69</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.378672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28939</th>\n",
       "      <td>trump gets snippy asked dial violent rhetoric ...</td>\n",
       "      <td>donald trump lost temper cnn jake tapper asked...</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>78</td>\n",
       "      <td>0.217949</td>\n",
       "      <td>0.439850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>poland's refusal accept muslim migrants may be...</td>\n",
       "      <td>warsaw  european commission decision launch so...</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>82</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.409091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42730</th>\n",
       "      <td>drunk refugees spit bite german nurses force h...</td>\n",
       "      <td>going end well germany hospital sigmaringen ge...</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>78</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2147</th>\n",
       "      <td>white house names hicks interim communications...</td>\n",
       "      <td>washington  longtime trump communications aide...</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>55</td>\n",
       "      <td>0.054545</td>\n",
       "      <td>0.347150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12574</th>\n",
       "      <td>qatar goes ahead billion typhoon combat jets d...</td>\n",
       "      <td>london  bae systems qatar entered contract val...</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>81</td>\n",
       "      <td>0.098765</td>\n",
       "      <td>0.369159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6632</th>\n",
       "      <td>f chief defends program trump criticism</td>\n",
       "      <td>washington  lockheed martin corps f fighter je...</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>48</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.397924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24058</th>\n",
       "      <td>gop rep wants ‘look good came charleston massa...</td>\n",
       "      <td>according gop congressman sean duffy need stop...</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>92</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>0.395894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42909</th>\n",
       "      <td>beyond evil th planned parenthood video stem c...</td>\n",
       "      <td>center medical progress warned us videos would...</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>134</td>\n",
       "      <td>0.261194</td>\n",
       "      <td>0.350602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4103</th>\n",
       "      <td>top us officials testify trumprussia probe reboot</td>\n",
       "      <td>washington  us house representatives intellige...</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>58</td>\n",
       "      <td>0.086207</td>\n",
       "      <td>0.332574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38369</th>\n",
       "      <td>marine arrested complaining government faceboo...</td>\n",
       "      <td>pretty surreal stuff four years since start op...</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>88</td>\n",
       "      <td>0.340909</td>\n",
       "      <td>0.417266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40218</th>\n",
       "      <td>breaking news facebook killer dead…here detail...</td>\n",
       "      <td>mi pa oh ny residents warned could anywhere st...</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>64</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.369718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8639</th>\n",
       "      <td>illinois republican lawmaker resigns cites fac...</td>\n",
       "      <td>chicago  prominent illinois republican resigne...</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>67</td>\n",
       "      <td>0.059701</td>\n",
       "      <td>0.377850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7599</th>\n",
       "      <td>us militia girds trouble presidential election...</td>\n",
       "      <td>jackson ga  georgia country road camouflaged m...</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>61</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>0.351921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1871</th>\n",
       "      <td>trump justice department civil rights nominee ...</td>\n",
       "      <td>washington  president donald trumps nominee le...</td>\n",
       "      <td>0.000557</td>\n",
       "      <td>65</td>\n",
       "      <td>0.061538</td>\n",
       "      <td>0.330544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18430</th>\n",
       "      <td>palestinian cabinet convenes gaza move reconci...</td>\n",
       "      <td>gaza  prime minister rami alhamdallah chaired ...</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>68</td>\n",
       "      <td>0.044118</td>\n",
       "      <td>0.349103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7877</th>\n",
       "      <td>last week clinton's white house chances percen...</td>\n",
       "      <td>new york  even sunday night's vicious presiden...</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>93</td>\n",
       "      <td>0.086022</td>\n",
       "      <td>0.363073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37619</th>\n",
       "      <td>judge napolitano three intel sources disclosed...</td>\n",
       "      <td>judge napolitano fox friends morning discussin...</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>85</td>\n",
       "      <td>0.458824</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5072</th>\n",
       "      <td>hawaii file first court challenge new trump tr...</td>\n",
       "      <td>san francisco  state hawaii said ask federal c...</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>62</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.349206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2163</th>\n",
       "      <td>key nafta talks 'not tearing apart worked' mexico</td>\n",
       "      <td>washington  mexicos economy minister ildefonso...</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>65</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.310345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38804</th>\n",
       "      <td>video rioters take selfies torched police car</td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "      <td>52</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6921</th>\n",
       "      <td>putin says trump clever understand new respons...</td>\n",
       "      <td>moscow  us presidentelect donald trump clever ...</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>61</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>0.387960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38984</th>\n",
       "      <td>fbi supervisor fired antitrump texts oversaw m...</td>\n",
       "      <td>supervisory special agent scrutiny removed rob...</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>83</td>\n",
       "      <td>0.228916</td>\n",
       "      <td>0.413011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27469</th>\n",
       "      <td>medical examiner finally releases report death...</td>\n",
       "      <td>world reeled beloved musical icon prince roger...</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>71</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>0.459064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16921</th>\n",
       "      <td>eu's tusk says leaders open internal work brex...</td>\n",
       "      <td>brussels  european council president donald tu...</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>64</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.364780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35665</th>\n",
       "      <td>judge jeanine pirro heads washington listening...</td>\n",
       "      <td>judge jeanine rips republican establishment yes</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>107</td>\n",
       "      <td>0.233645</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24152</th>\n",
       "      <td>watch senator al franken ripped ted cruz new o...</td>\n",
       "      <td>ted cruz accused senator al franken lying jeff...</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>97</td>\n",
       "      <td>0.216495</td>\n",
       "      <td>0.432836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43095</th>\n",
       "      <td>hillary couldnt find women buy tickets ‘women ...</td>\n",
       "      <td>hillary formidable opponent comes women voters...</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>103</td>\n",
       "      <td>0.533981</td>\n",
       "      <td>0.329480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18983</th>\n",
       "      <td>zimbabwe court frees activist pastor arrested ...</td>\n",
       "      <td>harare  zimbabwean court tuesday freed activis...</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>60</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.369942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32230</th>\n",
       "      <td>oreilly blasts fox news liberal murdoch brothe...</td>\n",
       "      <td>looks like bill reilly getting last laugh fox ...</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>95</td>\n",
       "      <td>0.305263</td>\n",
       "      <td>0.322997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17089</th>\n",
       "      <td>catalan foreign affairs chief says planning re...</td>\n",
       "      <td>brussels  catalonia regional government planni...</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>71</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.340909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14650</th>\n",
       "      <td>turkey detains suspected ties coup plotters ag...</td>\n",
       "      <td>istanbul  turkey detained people including for...</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>62</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.327660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39512</th>\n",
       "      <td>watch robin williams calls hypocrisy audience ...</td>\n",
       "      <td>robin williams oneofakind comedian brand humor...</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>132</td>\n",
       "      <td>0.257576</td>\n",
       "      <td>0.402878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15430</th>\n",
       "      <td>eu eyes tough brexit transition terms</td>\n",
       "      <td>brussels  eu diplomats start sketching brexit ...</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>37</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>0.411095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14935</th>\n",
       "      <td>greens hold climate german coalition talks</td>\n",
       "      <td>berlin  environmental policy dominated negotia...</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>52</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.373494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20757</th>\n",
       "      <td>indian court sentences two mumbai blasts convi...</td>\n",
       "      <td>mumbai  indian court thursday sentenced death ...</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>63</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.397590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41993</th>\n",
       "      <td>mom hears yr old daugher scream girls bathroom...</td>\n",
       "      <td>story anyone thinks king obama latest decree r...</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>109</td>\n",
       "      <td>0.302752</td>\n",
       "      <td>0.444840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32103</th>\n",
       "      <td>wow london mayor hammered piers morgan terrori...</td>\n",
       "      <td>awesome london mayor sadiq khan drilled piers ...</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>83</td>\n",
       "      <td>0.240964</td>\n",
       "      <td>0.389831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30403</th>\n",
       "      <td>paul krugman obama rolled back ronald reagans ...</td>\n",
       "      <td>paul krugman admission drooling release irs ta...</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>70</td>\n",
       "      <td>0.328571</td>\n",
       "      <td>0.418733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21243</th>\n",
       "      <td>guatemala top court sides un graft unit fight ...</td>\n",
       "      <td>guatemala city  guatemala top court tuesday ru...</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>70</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.408451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42613</th>\n",
       "      <td>oops nasa makes shocking claim burning fossil ...</td>\n",
       "      <td>plan electricity rates would necessarily skyro...</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>68</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.339196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43567</th>\n",
       "      <td>collapsing ‘russia hack witch hunt end well co...</td>\n",
       "      <td>st century wire says washington russian witch ...</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>87</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.416507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>jimmy carter recovers dehydration scare canada</td>\n",
       "      <td>winnipeg manitoba  former us president jimmy c...</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>54</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.387471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31428 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             title_clean  \\\n",
       "33958  breaking finally new wikileaks email…we going ...   \n",
       "19813  german liberals would expect finance ministry ...   \n",
       "25814  trump loses complete nervous breakdown worst w...   \n",
       "18689    merkel macron pledge lead eu forward postbrexit   \n",
       "44673     american tragedy really killed jonbenét ramsey   \n",
       "22749  ‘portal hell internet loses mysterious red lig...   \n",
       "2628   man whose firm behind trump dossier testify se...   \n",
       "10117  biden ukraine's poroshenko meet thursday white...   \n",
       "4388   us strikes syria show resolve chemical attacks...   \n",
       "16056  tunnel collapse may killed north korea nuclear...   \n",
       "964    manafort attorney evidence client colluded russia   \n",
       "12370  irish parliament committee backs referendum ab...   \n",
       "41275  sean hannity takes gloves hillary supporter me...   \n",
       "2270   trump administration's africa policy focus ago...   \n",
       "32755  crooked hillarys campaign manager wont rule ru...   \n",
       "31518  leftist antifa attacks boston police…shows tru...   \n",
       "6615         factbox trump fills top jobs administration   \n",
       "18256  venezuela's unrest food scarcity take psycholo...   \n",
       "28939  trump gets snippy asked dial violent rhetoric ...   \n",
       "11822  poland's refusal accept muslim migrants may be...   \n",
       "42730  drunk refugees spit bite german nurses force h...   \n",
       "2147   white house names hicks interim communications...   \n",
       "12574  qatar goes ahead billion typhoon combat jets d...   \n",
       "6632             f chief defends program trump criticism   \n",
       "24058  gop rep wants ‘look good came charleston massa...   \n",
       "42909  beyond evil th planned parenthood video stem c...   \n",
       "4103   top us officials testify trumprussia probe reboot   \n",
       "38369  marine arrested complaining government faceboo...   \n",
       "40218  breaking news facebook killer dead…here detail...   \n",
       "8639   illinois republican lawmaker resigns cites fac...   \n",
       "...                                                  ...   \n",
       "7599   us militia girds trouble presidential election...   \n",
       "1871   trump justice department civil rights nominee ...   \n",
       "18430  palestinian cabinet convenes gaza move reconci...   \n",
       "7877   last week clinton's white house chances percen...   \n",
       "37619  judge napolitano three intel sources disclosed...   \n",
       "5072   hawaii file first court challenge new trump tr...   \n",
       "2163   key nafta talks 'not tearing apart worked' mexico   \n",
       "38804      video rioters take selfies torched police car   \n",
       "6921   putin says trump clever understand new respons...   \n",
       "38984  fbi supervisor fired antitrump texts oversaw m...   \n",
       "27469  medical examiner finally releases report death...   \n",
       "16921  eu's tusk says leaders open internal work brex...   \n",
       "35665  judge jeanine pirro heads washington listening...   \n",
       "24152  watch senator al franken ripped ted cruz new o...   \n",
       "43095  hillary couldnt find women buy tickets ‘women ...   \n",
       "18983  zimbabwe court frees activist pastor arrested ...   \n",
       "32230  oreilly blasts fox news liberal murdoch brothe...   \n",
       "17089  catalan foreign affairs chief says planning re...   \n",
       "14650  turkey detains suspected ties coup plotters ag...   \n",
       "39512  watch robin williams calls hypocrisy audience ...   \n",
       "15430              eu eyes tough brexit transition terms   \n",
       "14935         greens hold climate german coalition talks   \n",
       "20757  indian court sentences two mumbai blasts convi...   \n",
       "41993  mom hears yr old daugher scream girls bathroom...   \n",
       "32103  wow london mayor hammered piers morgan terrori...   \n",
       "30403  paul krugman obama rolled back ronald reagans ...   \n",
       "21243  guatemala top court sides un graft unit fight ...   \n",
       "42613  oops nasa makes shocking claim burning fossil ...   \n",
       "43567  collapsing ‘russia hack witch hunt end well co...   \n",
       "2732      jimmy carter recovers dehydration scare canada   \n",
       "\n",
       "                              text_clean_without_reuters  text_typo_ratio  \\\n",
       "33958  latest wikileaks email evidence smoke hillary ...         0.000601   \n",
       "19813  berlin  germany free democrats fdp would want ...         0.000891   \n",
       "25814  trump bad week first humiliated front millions...         0.000223   \n",
       "18689  tallinn  french president emmanuel macron back...         0.000624   \n",
       "44673  roses know thorns hurt quote attributed jonben...         0.005435   \n",
       "22749  earlier month sinkhole opened outside donald t...         0.001782   \n",
       "2628   washington  cofounder firm commissioned dossie...         0.000067   \n",
       "10117  washington  us vice president joe biden ukrain...         0.000067   \n",
       "4388   brussels  european council president donald tu...         0.000000   \n",
       "16056  tokyo  tunnel north korea nuclear test site co...         0.000089   \n",
       "964    washington  attorney former trump campaign man...         0.000156   \n",
       "12370  dublin  irish parliamentary committee wednesda...         0.000134   \n",
       "41275  sean hannity taking page trump playbook member...         0.000445   \n",
       "2270   washington  trump administrations trade agenda...         0.000601   \n",
       "32755  mere months receiving end one stunning losses ...         0.000178   \n",
       "31518  hate antifa well left really want associate vi...         0.000356   \n",
       "6615    us presidentelect donald trump wednesday name...         0.001403   \n",
       "18256  los teques venezuela  venezuelan siblings jere...         0.000401   \n",
       "28939  donald trump lost temper cnn jake tapper asked...         0.000490   \n",
       "11822  warsaw  european commission decision launch so...         0.000111   \n",
       "42730  going end well germany hospital sigmaringen ge...         0.000334   \n",
       "2147   washington  longtime trump communications aide...         0.000111   \n",
       "12574  london  bae systems qatar entered contract val...         0.000200   \n",
       "6632   washington  lockheed martin corps f fighter je...         0.000089   \n",
       "24058  according gop congressman sean duffy need stop...         0.000379   \n",
       "42909  center medical progress warned us videos would...         0.000713   \n",
       "4103   washington  us house representatives intellige...         0.000356   \n",
       "38369  pretty surreal stuff four years since start op...         0.000111   \n",
       "40218  mi pa oh ny residents warned could anywhere st...         0.000290   \n",
       "8639   chicago  prominent illinois republican resigne...         0.000290   \n",
       "...                                                  ...              ...   \n",
       "7599   jackson ga  georgia country road camouflaged m...         0.000312   \n",
       "1871   washington  president donald trumps nominee le...         0.000557   \n",
       "18430  gaza  prime minister rami alhamdallah chaired ...         0.000601   \n",
       "7877   new york  even sunday night's vicious presiden...         0.000312   \n",
       "37619  judge napolitano fox friends morning discussin...         0.000223   \n",
       "5072   san francisco  state hawaii said ask federal c...         0.000111   \n",
       "2163   washington  mexicos economy minister ildefonso...         0.000089   \n",
       "38804                                                            0.000000   \n",
       "6921   moscow  us presidentelect donald trump clever ...         0.000290   \n",
       "38984  supervisory special agent scrutiny removed rob...         0.000757   \n",
       "27469  world reeled beloved musical icon prince roger...         0.000245   \n",
       "16921  brussels  european council president donald tu...         0.000089   \n",
       "35665    judge jeanine rips republican establishment yes         0.000000   \n",
       "24152  ted cruz accused senator al franken lying jeff...         0.000512   \n",
       "43095  hillary formidable opponent comes women voters...         0.000401   \n",
       "18983  harare  zimbabwean court tuesday freed activis...         0.000111   \n",
       "32230  looks like bill reilly getting last laugh fox ...         0.000624   \n",
       "17089  brussels  catalonia regional government planni...         0.000022   \n",
       "14650  istanbul  turkey detained people including for...         0.000312   \n",
       "39512  robin williams oneofakind comedian brand humor...         0.000200   \n",
       "15430  brussels  eu diplomats start sketching brexit ...         0.000223   \n",
       "14935  berlin  environmental policy dominated negotia...         0.000134   \n",
       "20757  mumbai  indian court thursday sentenced death ...         0.000356   \n",
       "41993  story anyone thinks king obama latest decree r...         0.000223   \n",
       "32103  awesome london mayor sadiq khan drilled piers ...         0.000713   \n",
       "30403  paul krugman admission drooling release irs ta...         0.000245   \n",
       "21243  guatemala city  guatemala top court tuesday ru...         0.000245   \n",
       "42613  plan electricity rates would necessarily skyro...         0.000401   \n",
       "43567  st century wire says washington russian witch ...         0.002294   \n",
       "2732   winnipeg manitoba  former us president jimmy c...         0.000134   \n",
       "\n",
       "       title_length_char  title_Upper_Ratio  text_stop_words_ratio  \n",
       "33958                 86           0.220930               0.420118  \n",
       "19813                 65           0.030769               0.386935  \n",
       "25814                 82           0.317073               0.409214  \n",
       "18689                 52           0.096154               0.382979  \n",
       "44673                 55           0.418182               0.384701  \n",
       "22749                 99           0.161616               0.291312  \n",
       "2628                  71           0.042254               0.422360  \n",
       "10117                 57           0.105263               0.347222  \n",
       "4388                  70           0.085714               0.254545  \n",
       "16056                 88           0.045455               0.330935  \n",
       "964                   58           0.051724               0.367647  \n",
       "12370                 70           0.014286               0.453165  \n",
       "41275                108           0.407407               0.413919  \n",
       "2270                  65           0.092308               0.370066  \n",
       "32755                134           0.253731               0.396226  \n",
       "31518                107           0.242991               0.126984  \n",
       "6615                  52           0.038462               0.333754  \n",
       "18256                 69           0.014493               0.378672  \n",
       "28939                 78           0.217949               0.439850  \n",
       "11822                 82           0.048780               0.409091  \n",
       "42730                 78           0.307692               0.400000  \n",
       "2147                  55           0.054545               0.347150  \n",
       "12574                 81           0.098765               0.369159  \n",
       "6632                  48           0.041667               0.397924  \n",
       "24058                 92           0.239130               0.395894  \n",
       "42909                134           0.261194               0.350602  \n",
       "4103                  58           0.086207               0.332574  \n",
       "38369                 88           0.340909               0.417266  \n",
       "40218                 64           0.375000               0.369718  \n",
       "8639                  67           0.059701               0.377850  \n",
       "...                  ...                ...                    ...  \n",
       "7599                  61           0.032787               0.351921  \n",
       "1871                  65           0.061538               0.330544  \n",
       "18430                 68           0.044118               0.349103  \n",
       "7877                  93           0.086022               0.363073  \n",
       "37619                 85           0.458824               0.375000  \n",
       "5072                  62           0.032258               0.349206  \n",
       "2163                  65           0.107692               0.310345  \n",
       "38804                 52           0.826923               0.000000  \n",
       "6921                  61           0.032787               0.387960  \n",
       "38984                 83           0.228916               0.413011  \n",
       "27469                 71           0.154930               0.459064  \n",
       "16921                 64           0.062500               0.364780  \n",
       "35665                107           0.233645               0.250000  \n",
       "24152                 97           0.216495               0.432836  \n",
       "43095                103           0.533981               0.329480  \n",
       "18983                 60           0.016667               0.369942  \n",
       "32230                 95           0.305263               0.322997  \n",
       "17089                 71           0.014085               0.340909  \n",
       "14650                 62           0.016129               0.327660  \n",
       "39512                132           0.257576               0.402878  \n",
       "15430                 37           0.081081               0.411095  \n",
       "14935                 52           0.038462               0.373494  \n",
       "20757                 63           0.031746               0.397590  \n",
       "41993                109           0.302752               0.444840  \n",
       "32103                 83           0.240964               0.389831  \n",
       "30403                 70           0.328571               0.418733  \n",
       "21243                 70           0.042857               0.408451  \n",
       "42613                 68           0.235294               0.339196  \n",
       "43567                 87           0.137931               0.416507  \n",
       "2732                  54           0.055556               0.387471  \n",
       "\n",
       "[31428 rows x 6 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "preprocessor_all_features = ColumnTransformer([\n",
    "    \n",
    "    ('vectorizer_title', CountVectorizer(), 'title_clean'),\n",
    "    ('vectorizer_text', CountVectorizer(), 'text_clean_without_reuters'),\n",
    "     ('scaling_title_char', MinMaxScaler(), ['title_length_char']),\n",
    "   \n",
    "    #insert function here\n",
    "])\n",
    "\n",
    "final_pipe_all_features = Pipeline([\n",
    "    ('preprocessing', preprocessor_all_features),\n",
    "    ('Logistic', LogisticRegression(solver = 'newton-cg', C =0.4 ))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\liamc\\.venvs\\lewagon\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('preprocessing', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('vectorizer_title', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', ...ty='l2', random_state=None, solver='newton-cg',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pipe_all_features.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def rem_urls(text):\n",
    "    return re.sub('https?:\\S+','',text)\n",
    "\n",
    "import string\n",
    "punc_no_sq = '!“#$%&\\()*+,./:;<=>?@[\\\\]^_`{|}~“”—’-'\n",
    "def remove_punctuation(text):\n",
    "    for punctuation in punc_no_sq:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text\n",
    "\n",
    "def remove_numbers(text):\n",
    "    text = ''.join(word for word in text if not word.isdigit())\n",
    "    return text\n",
    "\n",
    "def lower_case(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return text.split()\n",
    "\n",
    "import enchant\n",
    "english = enchant.DictWithPWL(\"en_US\", \"vocab.txt\")\n",
    "wrong_words={}\n",
    "correct_words=set()\n",
    "def get_typos_t(tokens):\n",
    "     wrong_count=0\n",
    "     for token in tokens:\n",
    "            if token in wrong_words:\n",
    "                wrong_words[token]+=1\n",
    "                wrong_count+=1\n",
    "            else:\n",
    "                if not token in correct_words:\n",
    "                    if token[0].islower() and not '-' in token and not english.check(token) and not english.check(token.capitalize()):\n",
    "                        wrong_words[token]=1\n",
    "                        wrong_count+=1\n",
    "                    else:\n",
    "                        correct_words.add(token)\n",
    "     return wrong_count    \n",
    "\n",
    "def clean_data(data):\n",
    "    \n",
    "    df = data.copy()\n",
    "    \n",
    "    #drop nan values in df\n",
    "    \n",
    "    df.dropna(axis = 0, inplace = True)\n",
    "    \n",
    "    #add title_clean and text_clean to df\n",
    "    \n",
    "    df['title_clean'] = df['title'].apply(rem_urls)\n",
    "    df['text_clean_without_reuters'] = df['text'].apply(rem_urls)\n",
    "    df['title_clean'] = df['title_clean'].apply(remove_punctuation)\n",
    "    df['text_clean_without_reuters'] = df['text_clean_without_reuters'].apply(remove_punctuation)\n",
    "    df['title_clean'] = df['title_clean'].apply(remove_numbers)\n",
    "    df['text_clean_without_reuters'] = df['text_clean_without_reuters'].apply(remove_numbers)\n",
    "    df['title_clean'] = df['title_clean'].apply(lower_case)\n",
    "    df['text_clean_without_reuters'] = df['text_clean_without_reuters'].apply(lower_case)\n",
    "    df['title_clean'] = df['title_clean'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop_words)]))\n",
    "    df['text_clean_without_reuters'] = df['text_clean_without_reuters'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop_words)]))\n",
    "    \n",
    "    #adding features to the df\n",
    "    \n",
    "    df['title_tokens'] = df['title_clean'].apply(tokenize_text)\n",
    "    df['text_tokens'] = df['text_clean_without_reuters'].apply(tokenize_text)\n",
    "    \n",
    "    df['typos_title_count'] = df['title_tokens'].apply(get_typos_t)\n",
    "    df['typos_text_count'] = df['text_tokens'].apply(get_typos_t)\n",
    "    \n",
    "    df['title_typo_ratio'] = df['typos_title_count']/len(df['title_tokens'])\n",
    "    df['text_typo_ratio'] = df['typos_text_count']/len(df['text_tokens'])\n",
    "    \n",
    "    df['title_length_char'] = data.title.str.len()\n",
    "    \n",
    "    df['title_Upper'] = df['title'].str.count(r'[A-Z]')\n",
    "    df['title_Upper_Ratio'] = df['title_Upper']/df['title_length_char']\n",
    "    \n",
    "    df['text_tokens_with_stopwords'] = df['text'].apply(tokenize_text)  \n",
    "    df['text_stop_words_ratio'] = df['text_tokens_with_stopwords'].apply(stopwords_ratio)\n",
    "    \n",
    "    return df[['title_clean','text_clean_without_reuters','text_typo_ratio', 'title_length_char', 'title_Upper_Ratio', 'text_stop_words_ratio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_test_new_york = pd.read_csv('../raw_data/new_york_real.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 4919, 1: 2884}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_with_all_features = final_pipe_all_features.predict(clean_data(first_test_new_york))\n",
    "unique_with_all_features, counts_with_all_features = np.unique(result_with_all_features, return_counts=True)\n",
    "dict(zip(unique_with_all_features, counts_with_all_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding more data for our model to generalize better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4702, 20)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "margarida_fake = pd.read_csv('../raw_data/fake_extra.csv')\n",
    "margarida_fake.dropna(inplace = True)\n",
    "margarida_fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 3)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_insider_true = pd.read_csv('../raw_data/Business_Insider.csv', nrows = 5000)\n",
    "business_insider_true.dropna(inplace = True)\n",
    "business_insider_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 3)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "washington_post_true = pd.read_csv('../raw_data/Washington_Post.csv', nrows = 5000)\n",
    "washington_post_true.dropna(inplace = True)\n",
    "washington_post_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "291.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
