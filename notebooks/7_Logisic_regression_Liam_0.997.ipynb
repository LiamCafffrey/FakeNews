{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re,string\n",
    "import tensorflow as tf\n",
    "import enchant\n",
    "import trafilatura\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "true = pd.read_csv('../raw_data/True.csv')\n",
    "fake = pd.read_csv('../raw_data/Fake.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting non necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "true.drop(columns = ['subject','date'], inplace = True)\n",
    "fake.drop(columns = ['subject','date'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief data cleaning to fake dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['/Getty Images']\n",
    "pat = '|'.join(r\"\\b{}\\b\".format(x) for x in stop_words)\n",
    "fake['text'] = fake['text'].str.replace(pat, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "true['score'] = 1\n",
    "fake['score'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate everything to one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = pd.concat([true,fake],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Cleaning some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing links from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def rem_urls(text):\n",
    "    return re.sub('https?:\\S+','',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['title_clean']=data['title'].apply(rem_urls)\n",
    "#data['text_clean']=data['text'].apply(rem_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "punc_no_sq = '!“#$%&\\()*+,./:;<=>?@[\\\\]^_`{|}~“”—’-'\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    for punctuation in punc_no_sq:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['title_clean']=data['title_clean'].apply(remove_punctuation)\n",
    "#data['text_clean']=data['text_clean'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    text = ''.join(word for word in text if not word.isdigit())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#data['title_clean']=data['title_clean'].apply(remove_numbers)\n",
    "\n",
    "#data['text_clean']=data['text_clean'].apply(remove_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making everything lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(text):\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['title_clean']=data['title_clean'].apply(lower_case)\n",
    "#data['text_clean']=data['text_clean'].apply(lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#data['title_clean'] = data['title_clean'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop_words)]))\n",
    "#data['text_clean'] = data['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizing text in order to count how many words we have to calculate ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    return text.split()\n",
    "\n",
    "#data['title_tokens']=data['title_clean'].apply(tokenize_text)\n",
    "#data['text_tokens']=data['text_clean'].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to count typos in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "english = enchant.DictWithPWL(\"en_US\", \"vocab.txt\")\n",
    "wrong_words={}\n",
    "correct_words=set()\n",
    "def get_typos_t(tokens):\n",
    "     wrong_count=0\n",
    "     for token in tokens:\n",
    "            if token in wrong_words:\n",
    "                wrong_words[token]+=1\n",
    "                wrong_count+=1\n",
    "            else:\n",
    "                if not token in correct_words:\n",
    "                    if token[0].islower() and not '-' in token and not english.check(token) and not english.check(token.capitalize()):\n",
    "                        wrong_words[token]=1\n",
    "                        wrong_count+=1\n",
    "                    else:\n",
    "                        correct_words.add(token)\n",
    "     return wrong_count    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Create typo ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#data['typos_title_count']=data['title_tokens'].apply(get_typos_t)\n",
    "#data['typos_text_count']=data['text_tokens'].apply(get_typos_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#data['title_typo_ratio']= data['typos_title_count']/len(data['title_tokens'])\n",
    "#data['text_typo_ratio']= data['typos_text_count']/len(data['text_tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining one cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data[['title', 'text', 'title_clean', 'text_clean', 'title_tokens',\n",
    "      # 'text_tokens', 'typos_title_count', 'typos_text_count',\n",
    "      # 'title_typo_ratio', 'text_typo_ratio','score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def rem_urls(text):\n",
    "    return re.sub('https?:\\S+','',text)\n",
    "\n",
    "import string\n",
    "punc_no_sq = '!“#$%&\\()*+,./:;<=>?@[\\\\]^_`{|}~“”—’-'\n",
    "def remove_punctuation(text):\n",
    "    for punctuation in punc_no_sq:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text\n",
    "\n",
    "def remove_numbers(text):\n",
    "    text = ''.join(word for word in text if not word.isdigit())\n",
    "    return text\n",
    "\n",
    "def lower_case(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return text.split()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def stopwords_ratio(tokens):\n",
    "    count_stop_words = 0\n",
    "    amount_tokens = 0   \n",
    "    for token in tokens:\n",
    "        amount_tokens += 1\n",
    "        if token in stop_words:\n",
    "            count_stop_words += 1\n",
    "    if amount_tokens == 0:\n",
    "        return 0\n",
    "    return count_stop_words / amount_tokens\n",
    "\n",
    "import enchant\n",
    "english = enchant.DictWithPWL(\"en_US\", \"vocab.txt\")\n",
    "wrong_words={}\n",
    "correct_words=set()\n",
    "def get_typos_t(tokens):\n",
    "     wrong_count=0\n",
    "     for token in tokens:\n",
    "            if token in wrong_words:\n",
    "                wrong_words[token]+=1\n",
    "                wrong_count+=1\n",
    "            else:\n",
    "                if not token in correct_words:\n",
    "                    if token[0].islower() and not '-' in token and not english.check(token) and not english.check(token.capitalize()):\n",
    "                        wrong_words[token]=1\n",
    "                        wrong_count+=1\n",
    "                    else:\n",
    "                        correct_words.add(token)\n",
    "     return wrong_count \n",
    "\n",
    "\n",
    "def clean_data(data):\n",
    "    \n",
    "    df = data.copy()\n",
    "    \n",
    "    #drop nan values in df\n",
    "    \n",
    "    df.dropna(axis = 0, inplace = True)\n",
    "    \n",
    "    #add title_clean and text_clean to df\n",
    "    stop_words = ['reuters']    \n",
    "    pat = '|'.join(r\"\\b{}\\b\".format(x) for x in stop_words)\n",
    "    \n",
    "    df['title_clean'] = df['title'].apply(rem_urls)\n",
    "    df['text_clean_without_reuters'] = df['text'].apply(rem_urls)\n",
    "    df['text_clean_without_reuters'] = df['text_clean_without_reuters'].str.replace(pat, '')\n",
    "    df['title_clean'] = df['title_clean'].apply(remove_punctuation)\n",
    "    df['text_clean_without_reuters'] = df['text_clean_without_reuters'].apply(remove_punctuation)\n",
    "    df['title_clean'] = df['title_clean'].apply(remove_numbers)\n",
    "    df['text_clean_without_reuters'] = df['text_clean_without_reuters'].apply(remove_numbers)\n",
    "    df['title_clean'] = df['title_clean'].apply(lower_case)\n",
    "    df['text_clean_without_reuters'] = df['text_clean_without_reuters'].apply(lower_case)\n",
    "    df['title_clean'] = df['title_clean'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop_words)]))\n",
    "    df['text_clean_without_reuters'] = df['text_clean_without_reuters'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop_words)]))\n",
    "    \n",
    "    #adding features to the df\n",
    "    \n",
    "    df['title_tokens'] = df['title_clean'].apply(tokenize_text)\n",
    "    df['text_tokens'] = df['text_clean_without_reuters'].apply(tokenize_text)\n",
    "    \n",
    "    df['text_stop_words_ratio'] = df['text_tokens'].apply(stopwords_ratio)\n",
    "    \n",
    "    df['typos_title_count'] = df['title_tokens'].apply(get_typos_t)\n",
    "    df['typos_text_count'] = df['text_tokens'].apply(get_typos_t)\n",
    "    \n",
    "    df['title_typo_ratio'] = df['typos_title_count']/len(df['title_tokens'])\n",
    "    df['text_typo_ratio'] = df['typos_text_count']/len(df['text_tokens'])\n",
    "    \n",
    "    df['title_length_char'] = data.title.str.len()\n",
    "    \n",
    "    df['title_Upper'] = df['title'].str.count(r'[A-Z]')\n",
    "    df['title_Upper_Ratio'] = df['title_Upper']/df['title_length_char']\n",
    "    \n",
    "    df['text_tokens_with_stopwords'] = df['text'].apply(tokenize_text)  \n",
    "    df['text_stop_words_ratio'] = df['text_tokens_with_stopwords'].apply(stopwords_ratio)\n",
    "    \n",
    "    return df[['title_clean','text_clean_without_reuters', 'text_typo_ratio', 'text_stop_words_ratio','title_length_char','typos_text_count', 'typos_title_count', 'title_tokens','text_tokens', 'title_Upper_Ratio', 'score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = clean_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to run a cross_val and find the best params for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## separating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[['title_clean', 'text_clean_without_reuters','text_typo_ratio']]\n",
    "y=data['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data in two : Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=0,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a pipeline to vectorize the text/title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "preprocessor = ColumnTransformer([\n",
    "    \n",
    "    ('vectorizer_title', CountVectorizer(), 'title_clean'),\n",
    "    ('vectorizer_text', CountVectorizer(), 'text_clean_without_reuters'),\n",
    "\n",
    "    #insert function here\n",
    "    \n",
    "])\n",
    "\n",
    "final_pipe = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('Logistic', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid searching for best params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    \n",
    "    'Logistic__solver': ('newton-cg', 'lbfgs', 'sag'),\n",
    "    'Logistic__C': ([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(final_pipe,\n",
    "                           parameters,\n",
    "                           scoring = [\"f1\", \"accuracy\", \"recall\"], \n",
    "                           refit= \"accuracy\",\n",
    "                           cv=3,\n",
    "                           verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\liamc\\.venvs\\lewagon\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\liamc\\.venvs\\lewagon\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\liamc\\.venvs\\lewagon\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\liamc\\.venvs\\lewagon\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "c:\\users\\liamc\\.venvs\\lewagon\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "grid_search.fit(x_train,y_train)\n",
    "stop = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic__C': 0.4, 'Logistic__solver': 'newton-cg'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ok so now that we have best params lets train a model on them**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model on best params found in grid_search with reuters in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\liamc\\.venvs\\lewagon\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('preprocessing', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('vectorizer_title', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', ...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pipe.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9939123979213066"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pipe.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_clean</th>\n",
       "      <th>text_clean_without_reuters</th>\n",
       "      <th>text_typo_ratio</th>\n",
       "      <th>text_stop_words_ratio</th>\n",
       "      <th>title_length_char</th>\n",
       "      <th>typos_text_count</th>\n",
       "      <th>typos_title_count</th>\n",
       "      <th>title_tokens</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>title_Upper_Ratio</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>as us budget fight looms republicans flip thei...</td>\n",
       "      <td>washington the head of a conservative republic...</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.357810</td>\n",
       "      <td>64</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>[as, us, budget, fight, looms, republicans, fl...</td>\n",
       "      <td>[washington, the, head, of, a, conservative, r...</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>us military to accept transgender recruits on ...</td>\n",
       "      <td>washington transgender people will be allowed ...</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.349359</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>[us, military, to, accept, transgender, recrui...</td>\n",
       "      <td>[washington, transgender, people, will, be, al...</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>senior us republican senator 'let mr mueller d...</td>\n",
       "      <td>washington the special counsel investigation o...</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.382932</td>\n",
       "      <td>60</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>[senior, us, republican, senator, 'let, mr, mu...</td>\n",
       "      <td>[washington, the, special, counsel, investigat...</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fbi russia probe helped by australian diplomat...</td>\n",
       "      <td>washington trump campaign adviser george papad...</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.361702</td>\n",
       "      <td>59</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>[fbi, russia, probe, helped, by, australian, d...</td>\n",
       "      <td>[washington, trump, campaign, adviser, george,...</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trump wants postal service to charge 'much mor...</td>\n",
       "      <td>seattlewashington president donald trump calle...</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.372066</td>\n",
       "      <td>69</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>[trump, wants, postal, service, to, charge, 'm...</td>\n",
       "      <td>[seattlewashington, president, donald, trump, ...</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>white house congress prepare for talks on spen...</td>\n",
       "      <td>west palm beach flawashington the white house ...</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.347245</td>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>[white, house, congress, prepare, for, talks, ...</td>\n",
       "      <td>[west, palm, beach, flawashington, the, white,...</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>trump says russia probe will be fair but timel...</td>\n",
       "      <td>west palm beach fla president donald trump sai...</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.371747</td>\n",
       "      <td>63</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>[trump, says, russia, probe, will, be, fair, b...</td>\n",
       "      <td>[west, palm, beach, fla, president, donald, tr...</td>\n",
       "      <td>0.079365</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>factbox trump on twitter dec approval rating a...</td>\n",
       "      <td>the following statements were posted to the ve...</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.318519</td>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>[factbox, trump, on, twitter, dec, approval, r...</td>\n",
       "      <td>[the, following, statements, were, posted, to,...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>trump on twitter dec global warming</td>\n",
       "      <td>the following statements were posted to the ve...</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>42</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[trump, on, twitter, dec, global, warming]</td>\n",
       "      <td>[the, following, statements, were, posted, to,...</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alabama official to certify senatorelect jones...</td>\n",
       "      <td>washington alabama secretary of state john mer...</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.358209</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[alabama, official, to, certify, senatorelect,...</td>\n",
       "      <td>[washington, alabama, secretary, of, state, jo...</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>jones certified us senate winner despite moore...</td>\n",
       "      <td>alabama officials on thursday certified democr...</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.389447</td>\n",
       "      <td>58</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[jones, certified, us, senate, winner, despite...</td>\n",
       "      <td>[alabama, officials, on, thursday, certified, ...</td>\n",
       "      <td>0.086207</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>new york governor questions the constitutional...</td>\n",
       "      <td>new yorkwashington the new us tax code targets...</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.321486</td>\n",
       "      <td>73</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>[new, york, governor, questions, the, constitu...</td>\n",
       "      <td>[new, yorkwashington, the, new, us, tax, code,...</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>factbox trump on twitter dec vanity fair hilla...</td>\n",
       "      <td>the following statements were posted to the ve...</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.373737</td>\n",
       "      <td>65</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>[factbox, trump, on, twitter, dec, vanity, fai...</td>\n",
       "      <td>[the, following, statements, were, posted, to,...</td>\n",
       "      <td>0.123077</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>trump on twitter dec trump iraq syria</td>\n",
       "      <td>the following statements were posted to the ve...</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.260417</td>\n",
       "      <td>46</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>[trump, on, twitter, dec, trump, iraq, syria]</td>\n",
       "      <td>[the, following, statements, were, posted, to,...</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>man says he delivered manure to mnuchin to pro...</td>\n",
       "      <td>in dec story in second paragraph corrects name...</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.353448</td>\n",
       "      <td>67</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>[man, says, he, delivered, manure, to, mnuchin...</td>\n",
       "      <td>[in, dec, story, in, second, paragraph, correc...</td>\n",
       "      <td>0.059701</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>virginia officials postpone lottery drawing to...</td>\n",
       "      <td>a lottery drawing to settle a tied virginia le...</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.364253</td>\n",
       "      <td>78</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>[virginia, officials, postpone, lottery, drawi...</td>\n",
       "      <td>[a, lottery, drawing, to, settle, a, tied, vir...</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>us lawmakers question businessman at trump tow...</td>\n",
       "      <td>washington a georgianamerican businessman who ...</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.373913</td>\n",
       "      <td>72</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>[us, lawmakers, question, businessman, at, tru...</td>\n",
       "      <td>[washington, a, georgianamerican, businessman,...</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>trump on twitter dec hillary clinton tax cut bill</td>\n",
       "      <td>the following statements were posted to the ve...</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.322034</td>\n",
       "      <td>57</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>[trump, on, twitter, dec, hillary, clinton, ta...</td>\n",
       "      <td>[the, following, statements, were, posted, to,...</td>\n",
       "      <td>0.140351</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>us appeals court rejects challenge to trump vo...</td>\n",
       "      <td>a us appeals court in washington on tuesday up...</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.326425</td>\n",
       "      <td>63</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>[us, appeals, court, rejects, challenge, to, t...</td>\n",
       "      <td>[a, us, appeals, court, in, washington, on, tu...</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>treasury secretary mnuchin was sent giftwrappe...</td>\n",
       "      <td>a giftwrapped package addressed to us treasury...</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.306220</td>\n",
       "      <td>77</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>[treasury, secretary, mnuchin, was, sent, gift...</td>\n",
       "      <td>[a, giftwrapped, package, addressed, to, us, t...</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>federal judge partially lifts trump's latest r...</td>\n",
       "      <td>washington a federal judge in seattle partiall...</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.348659</td>\n",
       "      <td>65</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>[federal, judge, partially, lifts, trump's, la...</td>\n",
       "      <td>[washington, a, federal, judge, in, seattle, p...</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>exclusive us memo weakens guidelines for prote...</td>\n",
       "      <td>new york the us justice department has issued ...</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.378319</td>\n",
       "      <td>82</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>[exclusive, us, memo, weakens, guidelines, for...</td>\n",
       "      <td>[new, york, the, us, justice, department, has,...</td>\n",
       "      <td>0.036585</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>trump travel ban should not apply to people wi...</td>\n",
       "      <td>a us appeals court on friday said president do...</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.356436</td>\n",
       "      <td>72</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[trump, travel, ban, should, not, apply, to, p...</td>\n",
       "      <td>[a, us, appeals, court, on, friday, said, pres...</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>second court rejects trump bid to stop transge...</td>\n",
       "      <td>washington a federal appeals court in washingt...</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.355742</td>\n",
       "      <td>68</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>[second, court, rejects, trump, bid, to, stop,...</td>\n",
       "      <td>[washington, a, federal, appeals, court, in, w...</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>failed vote to oust president shakes up peru's...</td>\n",
       "      <td>lima perus president pedro pablo kuczynski cou...</td>\n",
       "      <td>0.001225</td>\n",
       "      <td>0.353198</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>[failed, vote, to, oust, president, shakes, up...</td>\n",
       "      <td>[lima, perus, president, pedro, pablo, kuczyns...</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>trump signs tax government spending bills into...</td>\n",
       "      <td>washington us president donald trump signed re...</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.332432</td>\n",
       "      <td>51</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>[trump, signs, tax, government, spending, bill...</td>\n",
       "      <td>[washington, us, president, donald, trump, sig...</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>companies have up to a year for new us tax bil...</td>\n",
       "      <td>washington us financial regulators said on fri...</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.327485</td>\n",
       "      <td>64</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[companies, have, up, to, a, year, for, new, u...</td>\n",
       "      <td>[washington, us, financial, regulators, said, ...</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>trump on twitter dec tax cut missile defense bill</td>\n",
       "      <td>the following statements were posted to the ve...</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.318519</td>\n",
       "      <td>57</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>[trump, on, twitter, dec, tax, cut, missile, d...</td>\n",
       "      <td>[the, following, statements, were, posted, to,...</td>\n",
       "      <td>0.087719</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mexico to review need for tax changes after us...</td>\n",
       "      <td>mexico city mexicos finance ministry will eval...</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.362319</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[mexico, to, review, need, for, tax, changes, ...</td>\n",
       "      <td>[mexico, city, mexicos, finance, ministry, wil...</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>senate leader mcconnell sees a more collegial</td>\n",
       "      <td>washington us senate majority leader mitch mcc...</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.361949</td>\n",
       "      <td>50</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>[senate, leader, mcconnell, sees, a, more, col...</td>\n",
       "      <td>[washington, us, senate, majority, leader, mit...</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44868</th>\n",
       "      <td>degrees kevin bacons cultural mantle shattered...</td>\n",
       "      <td>st century wire says unless you have been livi...</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.405941</td>\n",
       "      <td>67</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>[degrees, kevin, bacons, cultural, mantle, sha...</td>\n",
       "      <td>[st, century, wire, says, unless, you, have, b...</td>\n",
       "      <td>0.119403</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44869</th>\n",
       "      <td>bernie sanders could end up winning iowa</td>\n",
       "      <td>st century wire says iowa s democratic party p...</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.361596</td>\n",
       "      <td>40</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>[bernie, sanders, could, end, up, winning, iowa]</td>\n",
       "      <td>[st, century, wire, says, iowa, s, democratic,...</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44870</th>\n",
       "      <td>plastic persona behind the scenes of the ted c...</td>\n",
       "      <td>st century wire says most people accept that a...</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.362637</td>\n",
       "      <td>64</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>[plastic, persona, behind, the, scenes, of, th...</td>\n",
       "      <td>[st, century, wire, says, most, people, accept...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44871</th>\n",
       "      <td>‘meet jeb – going for your sympathy vote</td>\n",
       "      <td>st century wire says as republican ted cruz an...</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.372222</td>\n",
       "      <td>41</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>[‘meet, jeb, –, going, for, your, sympathy, vote]</td>\n",
       "      <td>[st, century, wire, says, as, republican, ted,...</td>\n",
       "      <td>0.170732</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44872</th>\n",
       "      <td>boiler room – examination exclamation excitati...</td>\n",
       "      <td>tune in to the alternate current radio network...</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.220690</td>\n",
       "      <td>73</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>[boiler, room, –, examination, exclamation, ex...</td>\n",
       "      <td>[tune, in, to, the, alternate, current, radio,...</td>\n",
       "      <td>0.219178</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44873</th>\n",
       "      <td>eyewash cia elites misleading employees indica...</td>\n",
       "      <td>st century wire says the cia is trying its bes...</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.402913</td>\n",
       "      <td>97</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>[eyewash, cia, elites, misleading, employees, ...</td>\n",
       "      <td>[st, century, wire, says, the, cia, is, trying...</td>\n",
       "      <td>0.164948</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44874</th>\n",
       "      <td>activist ‘this is where you can make the most ...</td>\n",
       "      <td>st century wire says if you ve been following ...</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.422330</td>\n",
       "      <td>54</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>[activist, ‘this, is, where, you, can, make, t...</td>\n",
       "      <td>[st, century, wire, says, if, you, ve, been, f...</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44875</th>\n",
       "      <td>episode – sunday wire ‘crisis of liberty with ...</td>\n",
       "      <td>episode of sunday wire show finally resumes th...</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.315126</td>\n",
       "      <td>87</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>[episode, –, sunday, wire, ‘crisis, of, libert...</td>\n",
       "      <td>[episode, of, sunday, wire, show, finally, res...</td>\n",
       "      <td>0.195402</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44876</th>\n",
       "      <td>fbi release oregon video footage depicting dea...</td>\n",
       "      <td>st century wire saysupdate at pm et due to pub...</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>95</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>[fbi, release, oregon, video, footage, depicti...</td>\n",
       "      <td>[st, century, wire, saysupdate, at, pm, et, du...</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44877</th>\n",
       "      <td>trial by youtube mainstream media use secondha...</td>\n",
       "      <td>patrick henningsen st century wirethere exists...</td>\n",
       "      <td>0.003452</td>\n",
       "      <td>0.416134</td>\n",
       "      <td>95</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>[trial, by, youtube, mainstream, media, use, s...</td>\n",
       "      <td>[patrick, henningsen, st, century, wirethere, ...</td>\n",
       "      <td>0.147368</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44878</th>\n",
       "      <td>report ‘federal government escalated the viole...</td>\n",
       "      <td>killed rancher and protest spokesman robert la...</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.377953</td>\n",
       "      <td>61</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>[report, ‘federal, government, escalated, the,...</td>\n",
       "      <td>[killed, rancher, and, protest, spokesman, rob...</td>\n",
       "      <td>0.180328</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44879</th>\n",
       "      <td>boiler room – oregon standoff cuddle parties g...</td>\n",
       "      <td>tune in to the alternate current radio network...</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.280172</td>\n",
       "      <td>70</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>[boiler, room, –, oregon, standoff, cuddle, pa...</td>\n",
       "      <td>[tune, in, to, the, alternate, current, radio,...</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44880</th>\n",
       "      <td>eyewitness says feds ambushed bundys shots fir...</td>\n",
       "      <td>patrick henningsen st century wire update last...</td>\n",
       "      <td>0.003363</td>\n",
       "      <td>0.396094</td>\n",
       "      <td>105</td>\n",
       "      <td>151</td>\n",
       "      <td>3</td>\n",
       "      <td>[eyewitness, says, feds, ambushed, bundys, sho...</td>\n",
       "      <td>[patrick, henningsen, st, century, wire, updat...</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44881</th>\n",
       "      <td>episode – sunday wire ‘you know the drill with...</td>\n",
       "      <td>episode of sunday wire show finally resumes th...</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.284314</td>\n",
       "      <td>87</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>[episode, –, sunday, wire, ‘you, know, the, dr...</td>\n",
       "      <td>[episode, of, sunday, wire, show, finally, res...</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44882</th>\n",
       "      <td>‘therell be boots on the ground us making nois...</td>\n",
       "      <td>st century wire says various parties in washin...</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.378947</td>\n",
       "      <td>88</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>[‘therell, be, boots, on, the, ground, us, mak...</td>\n",
       "      <td>[st, century, wire, says, various, parties, in...</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44883</th>\n",
       "      <td>boston brakes how to hack a new car with your ...</td>\n",
       "      <td>st century wire says for those who still refus...</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.386059</td>\n",
       "      <td>64</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>[boston, brakes, how, to, hack, a, new, car, w...</td>\n",
       "      <td>[st, century, wire, says, for, those, who, sti...</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44884</th>\n",
       "      <td>oregon governor says feds ‘must act against pr...</td>\n",
       "      <td>st century wire says so far after nearly days ...</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>81</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>[oregon, governor, says, feds, ‘must, act, aga...</td>\n",
       "      <td>[st, century, wire, says, so, far, after, near...</td>\n",
       "      <td>0.135802</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44885</th>\n",
       "      <td>ron paul on burns oregon standoff and jury nul...</td>\n",
       "      <td>st century wire says if you ve been following ...</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.368932</td>\n",
       "      <td>79</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>[ron, paul, on, burns, oregon, standoff, and, ...</td>\n",
       "      <td>[st, century, wire, says, if, you, ve, been, f...</td>\n",
       "      <td>0.113924</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44886</th>\n",
       "      <td>boiler room as the frogs slowly boil – ep</td>\n",
       "      <td>tune in to the alternate current radio network...</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.275676</td>\n",
       "      <td>46</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>[boiler, room, as, the, frogs, slowly, boil, –...</td>\n",
       "      <td>[tune, in, to, the, alternate, current, radio,...</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44887</th>\n",
       "      <td>arizona rancher protesting in oregon is target...</td>\n",
       "      <td>rtone of the most visible members of the armed...</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.396774</td>\n",
       "      <td>89</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>[arizona, rancher, protesting, in, oregon, is,...</td>\n",
       "      <td>[rtone, of, the, most, visible, members, of, t...</td>\n",
       "      <td>0.134831</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44888</th>\n",
       "      <td>seven iranians freed in the prisoner swap have...</td>\n",
       "      <td>st century wire says this week the historic in...</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.420849</td>\n",
       "      <td>67</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>[seven, iranians, freed, in, the, prisoner, sw...</td>\n",
       "      <td>[st, century, wire, says, this, week, the, his...</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44889</th>\n",
       "      <td>hashtag hell the fake left</td>\n",
       "      <td>by dady chery and gilbert mercierall writers w...</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.410276</td>\n",
       "      <td>29</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>[hashtag, hell, the, fake, left]</td>\n",
       "      <td>[by, dady, chery, and, gilbert, mercierall, wr...</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44890</th>\n",
       "      <td>astroturfing journalist reveals brainwashing t...</td>\n",
       "      <td>vic bishop waking timesour reality is carefull...</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.366733</td>\n",
       "      <td>86</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>[astroturfing, journalist, reveals, brainwashi...</td>\n",
       "      <td>[vic, bishop, waking, timesour, reality, is, c...</td>\n",
       "      <td>0.104651</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44891</th>\n",
       "      <td>the new american century an era of fraud</td>\n",
       "      <td>paul craig robertsin the last years of the th ...</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.396373</td>\n",
       "      <td>41</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>[the, new, american, century, an, era, of, fraud]</td>\n",
       "      <td>[paul, craig, robertsin, the, last, years, of,...</td>\n",
       "      <td>0.170732</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44892</th>\n",
       "      <td>hillary clinton ‘israel first and no peace for...</td>\n",
       "      <td>robert fantina counterpunchalthough the united...</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.382008</td>\n",
       "      <td>62</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>[hillary, clinton, ‘israel, first, and, no, pe...</td>\n",
       "      <td>[robert, fantina, counterpunchalthough, the, u...</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44893</th>\n",
       "      <td>mcpain john mccain furious that iran treated u...</td>\n",
       "      <td>st century wire says as wire reported earlier ...</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>61</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>[mcpain, john, mccain, furious, that, iran, tr...</td>\n",
       "      <td>[st, century, wire, says, as, wire, reported, ...</td>\n",
       "      <td>0.213115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44894</th>\n",
       "      <td>justice yahoo settles email privacy classactio...</td>\n",
       "      <td>st century wire says it s a familiar theme whe...</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.420875</td>\n",
       "      <td>81</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>[justice, yahoo, settles, email, privacy, clas...</td>\n",
       "      <td>[st, century, wire, says, it, s, a, familiar, ...</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44895</th>\n",
       "      <td>sunnistan us and allied ‘safe zone plan to tak...</td>\n",
       "      <td>patrick henningsen st century wireremember whe...</td>\n",
       "      <td>0.004143</td>\n",
       "      <td>0.410453</td>\n",
       "      <td>85</td>\n",
       "      <td>186</td>\n",
       "      <td>1</td>\n",
       "      <td>[sunnistan, us, and, allied, ‘safe, zone, plan...</td>\n",
       "      <td>[patrick, henningsen, st, century, wireremembe...</td>\n",
       "      <td>0.141176</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44896</th>\n",
       "      <td>how to blow million al jazeera america finally...</td>\n",
       "      <td>st century wire says al jazeera america will g...</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>0.370614</td>\n",
       "      <td>67</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>[how, to, blow, million, al, jazeera, america,...</td>\n",
       "      <td>[st, century, wire, says, al, jazeera, america...</td>\n",
       "      <td>0.134328</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44897</th>\n",
       "      <td>us navy sailors held by iranian military – sig...</td>\n",
       "      <td>st century wire says as wire predicted in its ...</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.378719</td>\n",
       "      <td>81</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>[us, navy, sailors, held, by, iranian, militar...</td>\n",
       "      <td>[st, century, wire, says, as, wire, predicted,...</td>\n",
       "      <td>0.135802</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44898 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             title_clean  \\\n",
       "0      as us budget fight looms republicans flip thei...   \n",
       "1      us military to accept transgender recruits on ...   \n",
       "2      senior us republican senator 'let mr mueller d...   \n",
       "3      fbi russia probe helped by australian diplomat...   \n",
       "4      trump wants postal service to charge 'much mor...   \n",
       "5      white house congress prepare for talks on spen...   \n",
       "6      trump says russia probe will be fair but timel...   \n",
       "7      factbox trump on twitter dec approval rating a...   \n",
       "8                    trump on twitter dec global warming   \n",
       "9      alabama official to certify senatorelect jones...   \n",
       "10     jones certified us senate winner despite moore...   \n",
       "11     new york governor questions the constitutional...   \n",
       "12     factbox trump on twitter dec vanity fair hilla...   \n",
       "13                 trump on twitter dec trump iraq syria   \n",
       "14     man says he delivered manure to mnuchin to pro...   \n",
       "15     virginia officials postpone lottery drawing to...   \n",
       "16     us lawmakers question businessman at trump tow...   \n",
       "17     trump on twitter dec hillary clinton tax cut bill   \n",
       "18     us appeals court rejects challenge to trump vo...   \n",
       "19     treasury secretary mnuchin was sent giftwrappe...   \n",
       "20     federal judge partially lifts trump's latest r...   \n",
       "21     exclusive us memo weakens guidelines for prote...   \n",
       "22     trump travel ban should not apply to people wi...   \n",
       "23     second court rejects trump bid to stop transge...   \n",
       "24     failed vote to oust president shakes up peru's...   \n",
       "25     trump signs tax government spending bills into...   \n",
       "26     companies have up to a year for new us tax bil...   \n",
       "27     trump on twitter dec tax cut missile defense bill   \n",
       "28     mexico to review need for tax changes after us...   \n",
       "29         senate leader mcconnell sees a more collegial   \n",
       "...                                                  ...   \n",
       "44868  degrees kevin bacons cultural mantle shattered...   \n",
       "44869           bernie sanders could end up winning iowa   \n",
       "44870  plastic persona behind the scenes of the ted c...   \n",
       "44871           ‘meet jeb – going for your sympathy vote   \n",
       "44872  boiler room – examination exclamation excitati...   \n",
       "44873  eyewash cia elites misleading employees indica...   \n",
       "44874  activist ‘this is where you can make the most ...   \n",
       "44875  episode – sunday wire ‘crisis of liberty with ...   \n",
       "44876  fbi release oregon video footage depicting dea...   \n",
       "44877  trial by youtube mainstream media use secondha...   \n",
       "44878  report ‘federal government escalated the viole...   \n",
       "44879  boiler room – oregon standoff cuddle parties g...   \n",
       "44880  eyewitness says feds ambushed bundys shots fir...   \n",
       "44881  episode – sunday wire ‘you know the drill with...   \n",
       "44882  ‘therell be boots on the ground us making nois...   \n",
       "44883  boston brakes how to hack a new car with your ...   \n",
       "44884  oregon governor says feds ‘must act against pr...   \n",
       "44885  ron paul on burns oregon standoff and jury nul...   \n",
       "44886          boiler room as the frogs slowly boil – ep   \n",
       "44887  arizona rancher protesting in oregon is target...   \n",
       "44888  seven iranians freed in the prisoner swap have...   \n",
       "44889                         hashtag hell the fake left   \n",
       "44890  astroturfing journalist reveals brainwashing t...   \n",
       "44891           the new american century an era of fraud   \n",
       "44892  hillary clinton ‘israel first and no peace for...   \n",
       "44893  mcpain john mccain furious that iran treated u...   \n",
       "44894  justice yahoo settles email privacy classactio...   \n",
       "44895  sunnistan us and allied ‘safe zone plan to tak...   \n",
       "44896  how to blow million al jazeera america finally...   \n",
       "44897  us navy sailors held by iranian military – sig...   \n",
       "\n",
       "                              text_clean_without_reuters  text_typo_ratio  \\\n",
       "0      washington the head of a conservative republic...         0.000334   \n",
       "1      washington transgender people will be allowed ...         0.000223   \n",
       "2      washington the special counsel investigation o...         0.000178   \n",
       "3      washington trump campaign adviser george papad...         0.000379   \n",
       "4      seattlewashington president donald trump calle...         0.000512   \n",
       "5      west palm beach flawashington the white house ...         0.000356   \n",
       "6      west palm beach fla president donald trump sai...         0.000200   \n",
       "7      the following statements were posted to the ve...         0.000156   \n",
       "8      the following statements were posted to the ve...         0.000134   \n",
       "9      washington alabama secretary of state john mer...         0.000067   \n",
       "10     alabama officials on thursday certified democr...         0.000134   \n",
       "11     new yorkwashington the new us tax code targets...         0.000646   \n",
       "12     the following statements were posted to the ve...         0.000178   \n",
       "13     the following statements were posted to the ve...         0.000223   \n",
       "14     in dec story in second paragraph corrects name...         0.000379   \n",
       "15     a lottery drawing to settle a tied virginia le...         0.000379   \n",
       "16     washington a georgianamerican businessman who ...         0.000802   \n",
       "17     the following statements were posted to the ve...         0.000223   \n",
       "18     a us appeals court in washington on tuesday up...         0.000178   \n",
       "19     a giftwrapped package addressed to us treasury...         0.000267   \n",
       "20     washington a federal judge in seattle partiall...         0.000356   \n",
       "21     new york the us justice department has issued ...         0.000290   \n",
       "22     a us appeals court on friday said president do...         0.000134   \n",
       "23     washington a federal appeals court in washingt...         0.000200   \n",
       "24     lima perus president pedro pablo kuczynski cou...         0.001225   \n",
       "25     washington us president donald trump signed re...         0.000200   \n",
       "26     washington us financial regulators said on fri...         0.000134   \n",
       "27     the following statements were posted to the ve...         0.000178   \n",
       "28     mexico city mexicos finance ministry will eval...         0.000045   \n",
       "29     washington us senate majority leader mitch mcc...         0.000423   \n",
       "...                                                  ...              ...   \n",
       "44868  st century wire says unless you have been livi...         0.000379   \n",
       "44869  st century wire says iowa s democratic party p...         0.000379   \n",
       "44870  st century wire says most people accept that a...         0.000245   \n",
       "44871  st century wire says as republican ted cruz an...         0.000290   \n",
       "44872  tune in to the alternate current radio network...         0.000290   \n",
       "44873  st century wire says the cia is trying its bes...         0.000512   \n",
       "44874  st century wire says if you ve been following ...         0.000178   \n",
       "44875  episode of sunday wire show finally resumes th...         0.000334   \n",
       "44876  st century wire saysupdate at pm et due to pub...         0.000646   \n",
       "44877  patrick henningsen st century wirethere exists...         0.003452   \n",
       "44878  killed rancher and protest spokesman robert la...         0.000780   \n",
       "44879  tune in to the alternate current radio network...         0.000445   \n",
       "44880  patrick henningsen st century wire update last...         0.003363   \n",
       "44881  episode of sunday wire show finally resumes th...         0.000356   \n",
       "44882  st century wire says various parties in washin...         0.000468   \n",
       "44883  st century wire says for those who still refus...         0.000334   \n",
       "44884  st century wire says so far after nearly days ...         0.000200   \n",
       "44885  st century wire says if you ve been following ...         0.000156   \n",
       "44886  tune in to the alternate current radio network...         0.000356   \n",
       "44887  rtone of the most visible members of the armed...         0.000290   \n",
       "44888  st century wire says this week the historic in...         0.000356   \n",
       "44889  by dady chery and gilbert mercierall writers w...         0.000713   \n",
       "44890  vic bishop waking timesour reality is carefull...         0.000646   \n",
       "44891  paul craig robertsin the last years of the th ...         0.000802   \n",
       "44892  robert fantina counterpunchalthough the united...         0.000490   \n",
       "44893  st century wire says as wire reported earlier ...         0.000601   \n",
       "44894  st century wire says it s a familiar theme whe...         0.000379   \n",
       "44895  patrick henningsen st century wireremember whe...         0.004143   \n",
       "44896  st century wire says al jazeera america will g...         0.000735   \n",
       "44897  st century wire says as wire predicted in its ...         0.000824   \n",
       "\n",
       "       text_stop_words_ratio  title_length_char  typos_text_count  \\\n",
       "0                   0.357810                 64                15   \n",
       "1                   0.349359                 64                10   \n",
       "2                   0.382932                 60                 8   \n",
       "3                   0.361702                 59                17   \n",
       "4                   0.372066                 69                23   \n",
       "5                   0.347245                 64                16   \n",
       "6                   0.371747                 63                 9   \n",
       "7                   0.318519                 60                 7   \n",
       "8                   0.310000                 42                 6   \n",
       "9                   0.358209                 76                 3   \n",
       "10                  0.389447                 58                 6   \n",
       "11                  0.321486                 73                29   \n",
       "12                  0.373737                 65                 8   \n",
       "13                  0.260417                 46                10   \n",
       "14                  0.353448                 67                17   \n",
       "15                  0.364253                 78                17   \n",
       "16                  0.373913                 72                36   \n",
       "17                  0.322034                 57                10   \n",
       "18                  0.326425                 63                 8   \n",
       "19                  0.306220                 77                12   \n",
       "20                  0.348659                 65                16   \n",
       "21                  0.378319                 82                13   \n",
       "22                  0.356436                 72                 6   \n",
       "23                  0.355742                 68                 9   \n",
       "24                  0.353198                 55                55   \n",
       "25                  0.332432                 51                 9   \n",
       "26                  0.327485                 64                 6   \n",
       "27                  0.318519                 57                 8   \n",
       "28                  0.362319                 64                 2   \n",
       "29                  0.361949                 50                19   \n",
       "...                      ...                ...               ...   \n",
       "44868               0.405941                 67                17   \n",
       "44869               0.361596                 40                17   \n",
       "44870               0.362637                 64                11   \n",
       "44871               0.372222                 41                13   \n",
       "44872               0.220690                 73                13   \n",
       "44873               0.402913                 97                23   \n",
       "44874               0.422330                 54                 8   \n",
       "44875               0.315126                 87                15   \n",
       "44876               0.428571                 95                29   \n",
       "44877               0.416134                 95               155   \n",
       "44878               0.377953                 61                35   \n",
       "44879               0.280172                 70                20   \n",
       "44880               0.396094                105               151   \n",
       "44881               0.284314                 87                16   \n",
       "44882               0.378947                 88                21   \n",
       "44883               0.386059                 64                15   \n",
       "44884               0.379310                 81                 9   \n",
       "44885               0.368932                 79                 7   \n",
       "44886               0.275676                 46                16   \n",
       "44887               0.396774                 89                13   \n",
       "44888               0.420849                 67                16   \n",
       "44889               0.410276                 29                32   \n",
       "44890               0.366733                 86                29   \n",
       "44891               0.396373                 41                36   \n",
       "44892               0.382008                 62                22   \n",
       "44893               0.388889                 61                27   \n",
       "44894               0.420875                 81                17   \n",
       "44895               0.410453                 85               186   \n",
       "44896               0.370614                 67                33   \n",
       "44897               0.378719                 81                37   \n",
       "\n",
       "       typos_title_count                                       title_tokens  \\\n",
       "0                      0  [as, us, budget, fight, looms, republicans, fl...   \n",
       "1                      0  [us, military, to, accept, transgender, recrui...   \n",
       "2                      1  [senior, us, republican, senator, 'let, mr, mu...   \n",
       "3                      3  [fbi, russia, probe, helped, by, australian, d...   \n",
       "4                      1  [trump, wants, postal, service, to, charge, 'm...   \n",
       "5                      0  [white, house, congress, prepare, for, talks, ...   \n",
       "6                      1  [trump, says, russia, probe, will, be, fair, b...   \n",
       "7                      1  [factbox, trump, on, twitter, dec, approval, r...   \n",
       "8                      0         [trump, on, twitter, dec, global, warming]   \n",
       "9                      2  [alabama, official, to, certify, senatorelect,...   \n",
       "10                     0  [jones, certified, us, senate, winner, despite...   \n",
       "11                     0  [new, york, governor, questions, the, constitu...   \n",
       "12                     1  [factbox, trump, on, twitter, dec, vanity, fai...   \n",
       "13                     0      [trump, on, twitter, dec, trump, iraq, syria]   \n",
       "14                     1  [man, says, he, delivered, manure, to, mnuchin...   \n",
       "15                     0  [virginia, officials, postpone, lottery, drawi...   \n",
       "16                     0  [us, lawmakers, question, businessman, at, tru...   \n",
       "17                     0  [trump, on, twitter, dec, hillary, clinton, ta...   \n",
       "18                     0  [us, appeals, court, rejects, challenge, to, t...   \n",
       "19                     2  [treasury, secretary, mnuchin, was, sent, gift...   \n",
       "20                     0  [federal, judge, partially, lifts, trump's, la...   \n",
       "21                     0  [exclusive, us, memo, weakens, guidelines, for...   \n",
       "22                     0  [trump, travel, ban, should, not, apply, to, p...   \n",
       "23                     0  [second, court, rejects, trump, bid, to, stop,...   \n",
       "24                     0  [failed, vote, to, oust, president, shakes, up...   \n",
       "25                     0  [trump, signs, tax, government, spending, bill...   \n",
       "26                     0  [companies, have, up, to, a, year, for, new, u...   \n",
       "27                     0  [trump, on, twitter, dec, tax, cut, missile, d...   \n",
       "28                     1  [mexico, to, review, need, for, tax, changes, ...   \n",
       "29                     1  [senate, leader, mcconnell, sees, a, more, col...   \n",
       "...                  ...                                                ...   \n",
       "44868                  2  [degrees, kevin, bacons, cultural, mantle, sha...   \n",
       "44869                  0   [bernie, sanders, could, end, up, winning, iowa]   \n",
       "44870                  0  [plastic, persona, behind, the, scenes, of, th...   \n",
       "44871                  1  [‘meet, jeb, –, going, for, your, sympathy, vote]   \n",
       "44872                  0  [boiler, room, –, examination, exclamation, ex...   \n",
       "44873                  1  [eyewash, cia, elites, misleading, employees, ...   \n",
       "44874                  0  [activist, ‘this, is, where, you, can, make, t...   \n",
       "44875                  1  [episode, –, sunday, wire, ‘crisis, of, libert...   \n",
       "44876                  3  [fbi, release, oregon, video, footage, depicti...   \n",
       "44877                  1  [trial, by, youtube, mainstream, media, use, s...   \n",
       "44878                  0  [report, ‘federal, government, escalated, the,...   \n",
       "44879                  0  [boiler, room, –, oregon, standoff, cuddle, pa...   \n",
       "44880                  3  [eyewitness, says, feds, ambushed, bundys, sho...   \n",
       "44881                  0  [episode, –, sunday, wire, ‘you, know, the, dr...   \n",
       "44882                  0  [‘therell, be, boots, on, the, ground, us, mak...   \n",
       "44883                  1  [boston, brakes, how, to, hack, a, new, car, w...   \n",
       "44884                  0  [oregon, governor, says, feds, ‘must, act, aga...   \n",
       "44885                  0  [ron, paul, on, burns, oregon, standoff, and, ...   \n",
       "44886                  0  [boiler, room, as, the, frogs, slowly, boil, –...   \n",
       "44887                  0  [arizona, rancher, protesting, in, oregon, is,...   \n",
       "44888                  0  [seven, iranians, freed, in, the, prisoner, sw...   \n",
       "44889                  0                   [hashtag, hell, the, fake, left]   \n",
       "44890                  1  [astroturfing, journalist, reveals, brainwashi...   \n",
       "44891                  0  [the, new, american, century, an, era, of, fraud]   \n",
       "44892                  0  [hillary, clinton, ‘israel, first, and, no, pe...   \n",
       "44893                  2  [mcpain, john, mccain, furious, that, iran, tr...   \n",
       "44894                  1  [justice, yahoo, settles, email, privacy, clas...   \n",
       "44895                  1  [sunnistan, us, and, allied, ‘safe, zone, plan...   \n",
       "44896                  1  [how, to, blow, million, al, jazeera, america,...   \n",
       "44897                  0  [us, navy, sailors, held, by, iranian, militar...   \n",
       "\n",
       "                                             text_tokens  title_Upper_Ratio  \\\n",
       "0      [washington, the, head, of, a, conservative, r...           0.062500   \n",
       "1      [washington, transgender, people, will, be, al...           0.062500   \n",
       "2      [washington, the, special, counsel, investigat...           0.116667   \n",
       "3      [washington, trump, campaign, adviser, george,...           0.135593   \n",
       "4      [seattlewashington, president, donald, trump, ...           0.057971   \n",
       "5      [west, palm, beach, flawashington, the, white,...           0.046875   \n",
       "6      [west, palm, beach, fla, president, donald, tr...           0.079365   \n",
       "7      [the, following, statements, were, posted, to,...           0.100000   \n",
       "8      [the, following, statements, were, posted, to,...           0.119048   \n",
       "9      [washington, alabama, secretary, of, state, jo...           0.078947   \n",
       "10     [alabama, officials, on, thursday, certified, ...           0.086207   \n",
       "11     [new, yorkwashington, the, new, us, tax, code,...           0.027397   \n",
       "12     [the, following, statements, were, posted, to,...           0.123077   \n",
       "13     [the, following, statements, were, posted, to,...           0.130435   \n",
       "14     [in, dec, story, in, second, paragraph, correc...           0.059701   \n",
       "15     [a, lottery, drawing, to, settle, a, tied, vir...           0.012821   \n",
       "16     [washington, a, georgianamerican, businessman,...           0.055556   \n",
       "17     [the, following, statements, were, posted, to,...           0.140351   \n",
       "18     [a, us, appeals, court, in, washington, on, tu...           0.047619   \n",
       "19     [a, giftwrapped, package, addressed, to, us, t...           0.038961   \n",
       "20     [washington, a, federal, judge, in, seattle, p...           0.030769   \n",
       "21     [new, york, the, us, justice, department, has,...           0.036585   \n",
       "22     [a, us, appeals, court, on, friday, said, pres...           0.041667   \n",
       "23     [washington, a, federal, appeals, court, in, w...           0.029412   \n",
       "24     [lima, perus, president, pedro, pablo, kuczyns...           0.036364   \n",
       "25     [washington, us, president, donald, trump, sig...           0.019608   \n",
       "26     [washington, us, financial, regulators, said, ...           0.093750   \n",
       "27     [the, following, statements, were, posted, to,...           0.087719   \n",
       "28     [mexico, city, mexicos, finance, ministry, wil...           0.046875   \n",
       "29     [washington, us, senate, majority, leader, mit...           0.060000   \n",
       "...                                                  ...                ...   \n",
       "44868  [st, century, wire, says, unless, you, have, b...           0.119403   \n",
       "44869  [st, century, wire, says, iowa, s, democratic,...           0.175000   \n",
       "44870  [st, century, wire, says, most, people, accept...           0.125000   \n",
       "44871  [st, century, wire, says, as, republican, ted,...           0.170732   \n",
       "44872  [tune, in, to, the, alternate, current, radio,...           0.219178   \n",
       "44873  [st, century, wire, says, the, cia, is, trying...           0.164948   \n",
       "44874  [st, century, wire, says, if, you, ve, been, f...           0.037037   \n",
       "44875  [episode, of, sunday, wire, show, finally, res...           0.195402   \n",
       "44876  [st, century, wire, saysupdate, at, pm, et, du...           0.157895   \n",
       "44877  [patrick, henningsen, st, century, wirethere, ...           0.147368   \n",
       "44878  [killed, rancher, and, protest, spokesman, rob...           0.180328   \n",
       "44879  [tune, in, to, the, alternate, current, radio,...           0.257143   \n",
       "44880  [patrick, henningsen, st, century, wire, updat...           0.133333   \n",
       "44881  [episode, of, sunday, wire, show, finally, res...           0.206897   \n",
       "44882  [st, century, wire, says, various, parties, in...           0.056818   \n",
       "44883  [st, century, wire, says, for, those, who, sti...           0.156250   \n",
       "44884  [st, century, wire, says, so, far, after, near...           0.135802   \n",
       "44885  [st, century, wire, says, if, you, ve, been, f...           0.113924   \n",
       "44886  [tune, in, to, the, alternate, current, radio,...           0.347826   \n",
       "44887  [rtone, of, the, most, visible, members, of, t...           0.134831   \n",
       "44888  [st, century, wire, says, this, week, the, his...           0.044776   \n",
       "44889  [by, dady, chery, and, gilbert, mercierall, wr...           0.172414   \n",
       "44890  [vic, bishop, waking, timesour, reality, is, c...           0.104651   \n",
       "44891  [paul, craig, robertsin, the, last, years, of,...           0.170732   \n",
       "44892  [robert, fantina, counterpunchalthough, the, u...           0.096774   \n",
       "44893  [st, century, wire, says, as, wire, reported, ...           0.213115   \n",
       "44894  [st, century, wire, says, it, s, a, familiar, ...           0.185185   \n",
       "44895  [patrick, henningsen, st, century, wireremembe...           0.141176   \n",
       "44896  [st, century, wire, says, al, jazeera, america...           0.134328   \n",
       "44897  [st, century, wire, says, as, wire, predicted,...           0.135802   \n",
       "\n",
       "       score  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  \n",
       "5          1  \n",
       "6          1  \n",
       "7          1  \n",
       "8          1  \n",
       "9          1  \n",
       "10         1  \n",
       "11         1  \n",
       "12         1  \n",
       "13         1  \n",
       "14         1  \n",
       "15         1  \n",
       "16         1  \n",
       "17         1  \n",
       "18         1  \n",
       "19         1  \n",
       "20         1  \n",
       "21         1  \n",
       "22         1  \n",
       "23         1  \n",
       "24         1  \n",
       "25         1  \n",
       "26         1  \n",
       "27         1  \n",
       "28         1  \n",
       "29         1  \n",
       "...      ...  \n",
       "44868      0  \n",
       "44869      0  \n",
       "44870      0  \n",
       "44871      0  \n",
       "44872      0  \n",
       "44873      0  \n",
       "44874      0  \n",
       "44875      0  \n",
       "44876      0  \n",
       "44877      0  \n",
       "44878      0  \n",
       "44879      0  \n",
       "44880      0  \n",
       "44881      0  \n",
       "44882      0  \n",
       "44883      0  \n",
       "44884      0  \n",
       "44885      0  \n",
       "44886      0  \n",
       "44887      0  \n",
       "44888      0  \n",
       "44889      0  \n",
       "44890      0  \n",
       "44891      0  \n",
       "44892      0  \n",
       "44893      0  \n",
       "44894      0  \n",
       "44895      0  \n",
       "44896      0  \n",
       "44897      0  \n",
       "\n",
       "[44898 rows x 11 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this final pipe is fitted with training data cointaing reuters in the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our model on some new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a function that cleans the test text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import trafilatura\n",
    "downloaded = trafilatura.fetch_url('https://www.rte.ie/news/ireland/2020/1121/1179617-zoo-funding/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test = trafilatura.extract(downloaded)\n",
    "title_test='€1.6m in funding secured to support zoo sector'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "punc = string.punctuation + '“' + '”' + '’' + '‘' +'—' +'€' + '❤'\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    for punctuation in punc:\n",
    "        text = text.replace(punctuation, '')\n",
    "    text = ''.join(word for word in text if not word.isdigit())\n",
    "    text = text.lower()\n",
    "    text_split = text.split()\n",
    "    for word in text_split:\n",
    "        if word in stop_words:\n",
    "            text_split.remove(word)\n",
    "    text_string = ''\n",
    "    for word in text_split:\n",
    "        text_string = text_string + ' ' + word\n",
    "    return text_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clean = clean_text(text_test)\n",
    "title_clean = clean_text(title_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a general function to calculate typo ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def typo_ratio(text):\n",
    "    text_split = text.split()\n",
    "    lenght = len(text_split)\n",
    "    typos = get_typos_t(text_split)\n",
    "    return typos/lenght"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the data as an input that the model accepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_series=pd.Series(text_clean)\n",
    "title_series=pd.Series(title_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df=pd.DataFrame({'title_clean':title_series,'text_clean':test_series, 'text_typo_ratio': typo_ratio(text_clean)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Column ordering must be equal for fit and for transform when using the remainder keyword",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-05badade6c9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfinal_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\liamc\\.venvs\\lewagon\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\liamc\\.venvs\\lewagon\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m                 \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\liamc\\.venvs\\lewagon\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    514\u001b[0m             if (n_cols_transform >= n_cols_fit and\n\u001b[0;32m    515\u001b[0m                     any(X.columns[:n_cols_fit] != self._df_columns)):\n\u001b[1;32m--> 516\u001b[1;33m                 raise ValueError('Column ordering must be equal for fit '\n\u001b[0m\u001b[0;32m    517\u001b[0m                                  \u001b[1;34m'and for transform when using the '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m                                  'remainder keyword')\n",
      "\u001b[1;31mValueError\u001b[0m: Column ordering must be equal for fit and for transform when using the remainder keyword"
     ]
    }
   ],
   "source": [
    "final_pipe.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how we use the model to predict on one specific example(this pipe is fitted with reuters text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**okay so this model is generalising alot better**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking how well the model generalise for fake news using the fake news only dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_test = pd.read_csv('../raw_data/fake_extra.csv')\n",
    "fake_test['score'] = 0\n",
    "fake_test = clean_data(fake_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_fake = final_pipe.predict(fake_test)\n",
    "unique_fake, counts_fake = np.unique(result_fake, return_counts=True)\n",
    "dict(zip(unique_fake, counts_fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.880476393024245"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4140/(4140+562)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the accuracy on fake dataset by the pipeline fitted without reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing pipeline on true news "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_test = pd.read_csv('../raw_data/new_york_real.csv')\n",
    "true_test['score'] = 1\n",
    "true_test = clean_data(true_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 4931, 1: 2872}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_true = final_pipe.predict(true_test)\n",
    "unique_true, counts_true = np.unique(result_true, return_counts=True)\n",
    "dict(zip(unique_true, counts_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3680635652953992"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2872/(4931+2872)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying a logistic regression with more features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating over different features that we saw were relevant to distinguish fake from real news with a better score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title_clean', 'text_clean_without_reuters', 'text_typo_ratio',\n",
       "       'text_stop_words_ratio', 'title_length_char', 'typos_text_count',\n",
       "       'typos_title_count', 'title_tokens', 'text_tokens', 'title_Upper_Ratio',\n",
       "       'score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_iter= data[['title_clean', 'text_clean_without_reuters','title_length_char','title_Upper_Ratio', 'text_typo_ratio','text_stop_words_ratio']]\n",
    "y_iter=data['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train_iter,x_test_iter,y_train_iter,y_test_iter =train_test_split(x_iter,y_iter,random_state=0,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def rem_urls(text):\n",
    "    return re.sub('https?:\\S+','',text)\n",
    "\n",
    "import string\n",
    "punc_no_sq = '!“#$%&\\()*+,./:;<=>?@[\\\\]^_`{|}~“”—’-'\n",
    "def remove_punctuation(text):\n",
    "    for punctuation in punc_no_sq:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text\n",
    "\n",
    "def remove_numbers(text):\n",
    "    text = ''.join(word for word in text if not word.isdigit())\n",
    "    return text\n",
    "\n",
    "def lower_case(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return text.split()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def stopwords_ratio(tokens):\n",
    "    count_stop_words = 0\n",
    "    amount_tokens = 0   \n",
    "    for token in tokens:\n",
    "        amount_tokens += 1\n",
    "        if token in stop_words:\n",
    "            count_stop_words += 1\n",
    "    if amount_tokens == 0:\n",
    "        return 0\n",
    "    return count_stop_words / amount_tokens\n",
    "\n",
    "import enchant\n",
    "english = enchant.DictWithPWL(\"en_US\", \"vocab.txt\")\n",
    "wrong_words={}\n",
    "correct_words=set()\n",
    "def get_typos_t(tokens):\n",
    "     wrong_count=0\n",
    "     for token in tokens:\n",
    "            if token in wrong_words:\n",
    "                wrong_words[token]+=1\n",
    "                wrong_count+=1\n",
    "            else:\n",
    "                if not token in correct_words:\n",
    "                    if token[0].islower() and not '-' in token and not english.check(token) and not english.check(token.capitalize()):\n",
    "                        wrong_words[token]=1\n",
    "                        wrong_count+=1\n",
    "                    else:\n",
    "                        correct_words.add(token)\n",
    "     return wrong_count \n",
    "\n",
    "\n",
    "def clean_data(data):\n",
    "    \n",
    "    df = data.copy()\n",
    "    \n",
    "    #drop nan values in df\n",
    "    \n",
    "    df.dropna(axis = 0, inplace = True)\n",
    "    \n",
    "    #add title_clean and text_clean to df\n",
    "    stop_words = ['reuters']    \n",
    "    pat = '|'.join(r\"\\b{}\\b\".format(x) for x in stop_words)\n",
    "    \n",
    "    df['title_clean'] = df['title'].apply(rem_urls)\n",
    "    df['text_clean_without_reuters'] = df['text'].apply(rem_urls)\n",
    "    df['text_clean_without_reuters'] = df['text_clean_without_reuters'].str.replace(pat, '')\n",
    "    df['title_clean'] = df['title_clean'].apply(remove_punctuation)\n",
    "    df['text_clean_without_reuters'] = df['text_clean_without_reuters'].apply(remove_punctuation)\n",
    "    df['title_clean'] = df['title_clean'].apply(remove_numbers)\n",
    "    df['text_clean_without_reuters'] = df['text_clean_without_reuters'].apply(remove_numbers)\n",
    "    df['title_clean'] = df['title_clean'].apply(lower_case)\n",
    "    df['text_clean_without_reuters'] = df['text_clean_without_reuters'].apply(lower_case)\n",
    "    df['title_clean'] = df['title_clean'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop_words)]))\n",
    "    df['text_clean_without_reuters'] = df['text_clean_without_reuters'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop_words)]))\n",
    "    \n",
    "    #adding features to the df\n",
    "    \n",
    "    df['title_tokens'] = df['title_clean'].apply(tokenize_text)\n",
    "    df['text_tokens'] = df['text_clean_without_reuters'].apply(tokenize_text)\n",
    "    \n",
    "    df['text_stop_words_ratio'] = df['text_tokens'].apply(stopwords_ratio)\n",
    "    \n",
    "    df['typos_title_count'] = df['title_tokens'].apply(get_typos_t)\n",
    "    df['typos_text_count'] = df['text_tokens'].apply(get_typos_t)\n",
    "    \n",
    "    df['title_typo_ratio'] = df['typos_title_count']/len(df['title_tokens'])\n",
    "    df['text_typo_ratio'] = df['typos_text_count']/len(df['text_tokens'])\n",
    "    \n",
    "    df['title_length_char'] = data.title.str.len()\n",
    "    \n",
    "    df['title_Upper'] = df['title'].str.count(r'[A-Z]')\n",
    "    df['title_Upper_Ratio'] = df['title_Upper']/df['title_length_char']\n",
    "    \n",
    "    df['text_tokens_with_stopwords'] = df['text'].apply(tokenize_text)  \n",
    "    df['text_stop_words_ratio'] = df['text_tokens_with_stopwords'].apply(stopwords_ratio)\n",
    "    \n",
    "    return df[['title_clean', 'text_clean_without_reuters','title_length_char','title_Upper_Ratio', 'text_typo_ratio','text_stop_words_ratio','score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train a model on the different features we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "preprocessor_iter = ColumnTransformer([\n",
    "    \n",
    "    ('vectorizer_title', CountVectorizer(), 'title_clean'),\n",
    "    ('vectorizer_text', CountVectorizer(), 'text_clean_without_reuters'),\n",
    "    ('scaling_title_char', MinMaxScaler(), ['title_length_char']),\n",
    "   \n",
    "    #insert function here\n",
    "])\n",
    "\n",
    "final_pipe_iter = Pipeline([\n",
    "    ('preprocessing', preprocessor_iter),\n",
    "    ('Logistic', LogisticRegression(solver = 'newton-cg', C =0.4 ))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\liamc\\.venvs\\lewagon\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('preprocessing', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('vectorizer_title', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', ...ty='l2', random_state=None, solver='newton-cg',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pipe_iter.fit(x_train_iter,y_train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_test_new_york = pd.read_csv('../raw_data/new_york_real.csv')\n",
    "first_test_new_york['score'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 4896, 1: 2907}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_iter = final_pipe_iter.predict(clean_data(first_test_new_york))\n",
    "unique_iter, counts_iter = np.unique(result_iter, return_counts=True)\n",
    "dict(zip(unique_iter, counts_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combinations tried so far :\n",
    "# all features - 2907 real news detected\n",
    "# no features - 2895 real news detected\n",
    "# title_Upper ratio alone - \n",
    "# title_len_char - \n",
    "#text_typo + title_Upper_Ratio - 2895\n",
    "#text_typo 2872 real news detected\n",
    "\n",
    "#title_len_char + title_Upper_Ratio, text_typo_ratio, text_stop_words_ratio = best features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding more data for our model to generalize better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "margarida_fake = pd.read_csv('../raw_data/fake_extra.csv')\n",
    "margarida_fake.dropna(inplace = True)\n",
    "margarida_fake['score'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "margarida_fake = clean_data(margarida_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_insider_true = pd.read_csv('../raw_data/Business_Insider.csv', nrows = 5000)\n",
    "business_insider_true.dropna(inplace = True)\n",
    "business_insider_true['score'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_insider_true = clean_data(business_insider_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "washington_post_true = pd.read_csv('../raw_data/Washington_Post.csv', nrows = 5000)\n",
    "washington_post_true.dropna(inplace = True)\n",
    "washington_post_true['score'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "washington_post_true = clean_data(washington_post_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = pd.concat([data,margarida_fake,business_insider_true,washington_post_true],ignore_index=True, sort = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new= data_new[['title_clean', 'text_clean_without_reuters','title_length_char','title_Upper_Ratio', 'text_typo_ratio','text_stop_words_ratio']]\n",
    "y_new=data_new['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train_new,x_test_new,y_train_new,y_test_new =train_test_split(x_new,y_new,random_state=0,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "preprocessor_new = ColumnTransformer([\n",
    "    \n",
    "    ('vectorizer_title', CountVectorizer(), 'title_clean'),\n",
    "    ('vectorizer_text', CountVectorizer(), 'text_clean_without_reuters'),\n",
    "    ('scaling_title_char', MinMaxScaler(), ['title_length_char']),\n",
    "   \n",
    "    #insert function here\n",
    "])\n",
    "\n",
    "final_pipe_new = Pipeline([\n",
    "    ('preprocessing', preprocessor_iter),\n",
    "    ('Logistic', LogisticRegression(solver = 'newton-cg', C =0.4 ))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\liamc\\.venvs\\lewagon\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('preprocessing', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('vectorizer_title', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', ...ty='l2', random_state=None, solver='newton-cg',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pipe_new.fit(x_train_new,y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardian_true = pd.read_csv('../raw_data/Guardian.csv')\n",
    "guardian_true['score'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardian_true = clean_data(guardian_true)\n",
    "guardian_true.drop(columns =['score'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = final_pipe_new.predict(guardian_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8471638655462185\n"
     ]
    }
   ],
   "source": [
    "count_1 = 0\n",
    "count = 0\n",
    "for pred in predict:\n",
    "    count += 1\n",
    "    if pred == 1:\n",
    "        count_1 += 1\n",
    "print(count_1/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fake = pd.read_csv('../raw_data/fake_new_test.csv')\n",
    "new_fake['score'] = 0\n",
    "new_fake = clean_data(new_fake)\n",
    "new_fake.drop(columns =['score'], inplace = True)\n",
    "predict = final_pipe_new.predict(new_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8445006321112516\n"
     ]
    }
   ],
   "source": [
    "count_0 = 0\n",
    "count = 0\n",
    "for pred in predict:\n",
    "    count += 1\n",
    "    if pred == 0:\n",
    "        count_0 += 1\n",
    "print(count_0/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_true_1 = pd.read_csv('../raw_data/CNN_test.csv')\n",
    "new_true_1['score'] = 1\n",
    "new_true_1 = clean_data(new_true_1)\n",
    "new_true_1.drop(columns =['score'], inplace = True)\n",
    "predict = final_pipe_new.predict(new_true_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8958043175487466\n"
     ]
    }
   ],
   "source": [
    "count_1 = 0\n",
    "count = 0\n",
    "for pred in predict:\n",
    "    count += 1\n",
    "    if pred == 1:\n",
    "        count_1 += 1\n",
    "print(count_1/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_true_2 = pd.read_csv('../raw_data/Vox.csv')\n",
    "new_true_2['score'] = 1\n",
    "new_true_2 = clean_data(new_true_2)\n",
    "new_true_2.drop(columns =['score'], inplace = True)\n",
    "predict = final_pipe_new.predict(new_true_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8502122498483929\n"
     ]
    }
   ],
   "source": [
    "count_1 = 0\n",
    "count = 0\n",
    "for pred in predict:\n",
    "    count += 1\n",
    "    if pred == 1:\n",
    "        count_1 += 1\n",
    "print(count_1/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fake_1 = pd.read_csv('../raw_data/FAKE_3.csv')\n",
    "new_fake_1['score'] = 0\n",
    "new_fake_1 = clean_data(new_fake_1)\n",
    "new_fake_1.drop(columns =['score'], inplace = True)\n",
    "predict = final_pipe_new.predict(new_fake_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16957605985037408\n"
     ]
    }
   ],
   "source": [
    "count_0 = 0\n",
    "count = 0\n",
    "for pred in predict:\n",
    "    count += 1\n",
    "    if pred == 0:\n",
    "        count_0 += 1\n",
    "print(count_0/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../raw_data/datasets_concat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[['title', 'text', 'score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(r'../raw_data/all_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../raw_data/all_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57183, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    29000\n",
       "0    28183\n",
       "Name: score, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = test[['title', 'text', 'score']]\n",
    "test.to_csv(r'../raw_data/working.csv', index = False)\n",
    "test = pd.read_csv('../raw_data/working.csv')\n",
    "test['score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "291.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
